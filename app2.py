# =====================================
# Streamlit Web App: æ¨¡æ‹ŸProjectï¼šäººäº‹ç”¨åˆåŒè®°å½•è¡¨è‡ªåŠ¨å®¡æ ¸ï¼ˆå››è¾“å‡ºè¡¨ç‰ˆ + æ¼å¡«æ£€æŸ¥ + é©»åº—å®¢æˆ·ç‰ˆï¼‰
# =====================================

import streamlit as st
import pandas as pd
import time
from openpyxl import load_workbook, Workbook
from openpyxl.styles import PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows # <--- æ·»åŠ æˆ–ç¡®ä¿è¿™ä¸€è¡Œå­˜åœ¨
from io import BytesIO

def normalize_contract_key(series: pd.Series) -> pd.Series:
    """
    å¯¹åˆåŒå· Series è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œç”¨äºå®‰å…¨çš„ pd.merge æ“ä½œã€‚
    """
    # 1. ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼ŒåŒæ—¶å¤„ç†ç¼ºå¤±å€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    s = series.astype(str)
    
    # 2. ç§»é™¤å¸¸è§çš„æµ®ç‚¹æ•°æ®‹ç•™ï¼ˆä»¥é˜²åŸå§‹æ•°æ®é”™è¯¯è¾“å…¥ï¼‰
    s = s.str.replace(r"\.0$", "", regex=True) 
    
    # 3. æ ¸å¿ƒï¼šç§»é™¤é¦–å°¾ç©ºæ ¼ï¼ˆå¤„ç†æœ€å¸¸è§çš„å¯¼å…¥é”™è¯¯ï¼‰
    s = s.str.strip()
    
    # 4. ç»Ÿä¸€è½¬æ¢ä¸ºå¤§å†™ï¼ˆå¤„ç†å¤§å°å†™ä¸ä¸€è‡´é—®é¢˜ï¼Œå¦‚ 'pazl' vs 'PAZL'ï¼‰
    s = s.str.upper() 
    
    # 5. å¤„ç†å…¨è§’/åŠè§’å·®å¼‚ï¼ˆå°†å¸¸è§çš„å…¨è§’è¿æ¥ç¬¦è½¬ä¸ºåŠè§’ï¼‰
    s = s.str.replace('ï¼', '-', regex=False) # å…¨è§’è¿æ¥ç¬¦è½¬åŠè§’
    
    # 6. å¤„ç†å…¶ä»–å¯èƒ½çš„ç©ºç™½å­—ç¬¦ï¼ˆä¾‹å¦‚ tabs, æ¢è¡Œç¬¦ç­‰ï¼‰
    s = s.str.replace(r'\s+', '', regex=True)
    
    return s

# =====================================
# ğŸ åº”ç”¨æ ‡é¢˜ä¸è¯´æ˜
# =====================================
st.title("ğŸ“Š æ¨¡æ‹Ÿäººäº‹ç”¨è–ªèµ„è®¡ç®—è¡¨è‡ªåŠ¨å®¡æ ¸ç³»ç»Ÿ-1é‡å¡")

st.image("image/app1(1).png")

# =====================================
# ğŸ“‚ ä¸Šä¼ æ–‡ä»¶åŒºï¼šè¦æ±‚ä¸Šä¼  4 ä¸ª xlsx æ–‡ä»¶
# =====================================
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼ æ–‡ä»¶åä¸­åŒ…å«ä»¥ä¸‹å­—æ®µçš„æ–‡ä»¶ï¼šæœˆé‡å¡ã€æ”¾æ¬¾æ˜ç»†ã€å­—æ®µã€äºŒæ¬¡æ˜ç»†ã€‚æœ€åèªŠå†™ï¼Œéœ€æ£€çš„è¡¨ä¸ºæ–‡ä»¶ååŒ…å«â€˜æœˆé‡å¡â€™å­—æ®µçš„è¡¨ã€‚",
    type="xlsx",
    accept_multiple_files=True
)

if not uploaded_files or len(uploaded_files) < 4:
    st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ 4 ä¸ªæ–‡ä»¶åç»§ç»­")
    st.stop()
else:
    st.success("âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ")

# =====================================
# ğŸ§° å·¥å…·å‡½æ•°åŒºï¼ˆæ–‡ä»¶å®šä½ã€åˆ—åæ¨¡ç³ŠåŒ¹é…ã€æ—¥æœŸ/æ•°å€¼å¤„ç†ï¼‰
# =====================================

# æŒ‰å…³é”®å­—æŸ¥æ‰¾æ–‡ä»¶ï¼ˆæ–‡ä»¶ååŒ…å«å…³é”®å­—ï¼‰
def find_file(files_list, keyword):
    for f in files_list:
        if keyword in f.name:
            return f
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„æ–‡ä»¶")

# ç»Ÿä¸€åˆ—åæ ¼å¼ï¼ˆå»ç©ºæ ¼ã€è½¬å°å†™ï¼‰
def normalize_colname(c): return str(c).strip().lower()

# æŒ‰å…³é”®å­—åŒ¹é…åˆ—åï¼ˆæ”¯æŒ exact ç²¾ç¡®åŒ¹é…ä¸æ¨¡ç³ŠåŒ¹é…ï¼‰
def find_col(df, keyword, exact=False):
    key = keyword.strip().lower()
    for col in df.columns:
        cname = normalize_colname(col)
        if (exact and cname == key) or (not exact and key in cname):
            return col
    return None

# æŸ¥æ‰¾sheetï¼ˆsheetååŒ…å«å…³é”®å­—å³å¯ï¼‰
def find_sheet(xls, keyword):
    for s in xls.sheet_names:
        if keyword in s:
            return s
    raise ValueError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„sheet")

# ç»Ÿä¸€æ•°å€¼è§£æï¼ˆå»é€—å·ã€è½¬floatã€å¤„ç†ç™¾åˆ†å·ï¼‰
def normalize_num(val):
    if pd.isna(val): return None
    s = str(val).replace(",", "").strip()
    if s in ["", "-", "nan"]: return None
    try:
        if "%" in s: return float(s.replace("%", "")) / 100
        return float(s)
    except ValueError:
        return s

# æ—¥æœŸåŒ¹é…ï¼ˆå¹´/æœˆ/æ—¥å®Œå…¨ä¸€è‡´ï¼‰
def same_date_ymd(a, b):
    try:
        da = pd.to_datetime(a, errors='coerce')
        db = pd.to_datetime(b, errors='coerce')
        if pd.isna(da) or pd.isna(db): return False
        return (da.year, da.month, da.day) == (db.year, db.month, db.day)
    except Exception:
        return False
def prepare_ref_df(ref_df, mapping, prefix):
    # --- ä¿®æ­£å¼€å§‹ ---
    
    # 1. æ‰¾åˆ°å‚è€ƒè¡¨(ref_df)ä¸­çš„â€œåˆåŒâ€åˆ—
    # æˆ‘ä»¬ä½¿ç”¨ find_colï¼Œè¿™æ‰æ˜¯æ­£ç¡®çš„åšæ³•
    contract_col = find_col(ref_df, "åˆåŒ") 
    
    # 2. å¦‚æœåœ¨ ref_df ä¸­æ‰¾ä¸åˆ°åˆåŒåˆ—ï¼Œåˆ™æ— æ³•ç»§ç»­
    if not contract_col:
        st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°'åˆåŒ'åˆ—ï¼Œè·³è¿‡æ­¤æ•°æ®æºã€‚")
        return pd.DataFrame(columns=['__KEY__']) # è¿”å›ä¸€ä¸ªç©ºçš„å¸¦keyçš„df
        
    std_df = pd.DataFrame()
    
    # 3. VVVV æ’å…¥å½’ä¸€åŒ–å‡½æ•° VVVV
    # ä½¿ç”¨æ‰¾åˆ°çš„ contract_col æ¥åº”ç”¨å½’ä¸€åŒ–
    std_df['__KEY__'] = normalize_contract_key(ref_df[contract_col])
    # ^^^^ æ’å…¥å½’ä¸€åŒ–å‡½æ•° ^^^^
    
    # --- ä¿®æ­£ç»“æŸ ---
    # 4. æå–å¹¶é‡å‘½åæ‰€æœ‰éœ€è¦çš„å­—æ®µ (ã€å·²ä¿®æ”¹ã€‘å¢åŠ ç§ŸèµæœŸé™*12çš„é€»è¾‘)
    for main_kw, ref_kw in mapping.items():
Â  Â  Â  Â  # åŸå¸‚ç»ç†éœ€è¦ç²¾ç¡®åŒ¹é…
Â  Â  Â  Â  exact = (main_kw == "åŸå¸‚ç»ç†")Â 
Â  Â  Â  Â  ref_col_name = find_col(ref_df, ref_kw, exact=exact)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if ref_col_name:
Â  Â  Â  Â  Â  Â  # --- VVVV (è¿™æ˜¯æ–°æ·»åŠ çš„é€»è¾‘) VVVV ---
Â  Â  Â  Â  Â  Â  # è·å–åŸå§‹æ•°æ® Series
Â  Â  Â  Â  Â  Â  s_ref_raw = ref_df[ref_col_name]
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # æ£€æŸ¥æ˜¯å¦æ˜¯ 'fk' è¡¨çš„ 'ç§ŸèµæœŸé™'
Â  Â  Â  Â  Â  Â  if prefix == 'fk' and main_kw == 'ç§ŸèµæœŸé™':
Â  Â  Â  Â  Â  Â  Â  Â  # åº”ç”¨è½¬æ¢ï¼šå¹´ -> æœˆ
Â  Â  Â  Â  Â  Â  Â  Â  s_ref_transformed = pd.to_numeric(s_ref_raw, errors='coerce') * 12
Â  Â  Â  Â  Â  Â  Â  Â  std_df[f'ref_{prefix}_{main_kw}'] = s_ref_transformed
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # æ— è½¬æ¢ï¼Œç›´æ¥èµ‹å€¼
Â  Â  Â  Â  Â  Â  Â  Â  std_df[f'ref_{prefix}_{main_kw}'] = s_ref_raw
Â  Â  Â  Â  Â  Â  # --- ^^^^ (æ–°é€»è¾‘ç»“æŸ) ^^^^ ---
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°åˆ— (main: '{main_kw}', ref: '{ref_kw}')")

    # 5. æ•ˆä»¿åŸå§‹é€»è¾‘ï¼šåªå–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹ (è¿™éƒ¨åˆ†é€»è¾‘ä¿æŒä¸å˜)
    std_df = std_df.drop_duplicates(subset=['__KEY__'], keep='first')
    return std_df

def compare_series_vec(s_main, s_ref, main_kw):
    """
    å‘é‡åŒ–æ¯”è¾ƒä¸¤ä¸ªSeriesï¼Œå¤åˆ»åŸå§‹çš„ compare_fields_and_mark é€»è¾‘ã€‚
    è¿”å›ä¸€ä¸ªå¸ƒå°”Seriesï¼ŒTrueè¡¨ç¤ºå­˜åœ¨å·®å¼‚ã€‚
    (V2ï¼šå¢åŠ å¯¹ merge å¤±è´¥ (NaN) çš„é™é»˜è·³è¿‡)
    """
    
    # 0. è¯†åˆ«çœŸæ­£çš„ "Merge å¤±è´¥" (s_ref æ˜¯ç‰©ç† NaN)
    #    æˆ‘ä»¬å¿…é¡»åœ¨ s_ref è¢« astype(str) æ±¡æŸ“å‰æ‰§è¡Œæ­¤æ“ä½œ
    merge_failed_mask = s_ref.isna() 

    # 1. é¢„å¤„ç†ï¼šå¤„ç†ç©ºå€¼ã€‚åŸå§‹é€»è¾‘ï¼šåŒä¸ºNaN/ç©ºåˆ™è®¤ä¸ºä¸€è‡´ã€‚
    main_is_na = pd.isna(s_main) | (s_main.astype(str).str.strip().isin(["", "nan", "None"]))
    ref_is_na = pd.isna(s_ref) | (s_ref.astype(str).str.strip().isin(["", "nan", "None"]))
    
    # ä¸¤è€…éƒ½ä¸ºç©ºï¼ˆNaN, "", "None"ç­‰ï¼‰ï¼Œä¸ç®—é”™è¯¯
    both_are_na = main_is_na & ref_is_na
    
    # 2. æ—¥æœŸå­—æ®µæ¯”è¾ƒ
    if any(k in main_kw for k in ["æ—¥æœŸ", "æ—¶é—´"]):
        d_main = pd.to_datetime(s_main, errors='coerce')
        d_ref = pd.to_datetime(s_ref, errors='coerce')
        
        valid_dates_mask = d_main.notna() & d_ref.notna()
        date_diff_mask = (d_main.dt.date != d_ref.dt.date)
        
        errors = valid_dates_mask & date_diff_mask
    
    # 3. æ•°å€¼/æ–‡æœ¬æ¯”è¾ƒ
    else:
        s_main_norm = s_main.apply(normalize_num)
        s_ref_norm = s_ref.apply(normalize_num)
        
        main_is_na_norm = pd.isna(s_main_norm) | (s_main_norm.astype(str).str.strip().isin(["", "nan", "None"]))
        ref_is_na_norm = pd.isna(s_ref_norm) | (s_ref_norm.astype(str).str.strip().isin(["", "nan", "None"]))
        both_are_na_norm = main_is_na_norm & ref_is_na_norm

        is_num_main = s_main_norm.apply(lambda x: isinstance(x, (int, float)))
        is_num_ref = s_ref_norm.apply(lambda x: isinstance(x, (int, float)))
        both_are_num = is_num_main & is_num_ref
        
        errors = pd.Series(False, index=s_main.index)

        # 3a. æ•°å€¼æ¯”è¾ƒ
Â  Â  Â  Â  if both_are_num.any():
Â  Â  Â  Â  Â  Â  num_main = s_main_norm[both_are_num].fillna(0) # fillna(0) for safety
Â  Â  Â  Â  Â  Â  num_ref = s_ref_norm[both_are_num].fillna(0)
Â  Â  Â  Â  Â  Â  diff = (num_main - num_ref).abs()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # --- VVVV (è¿™æ˜¯ä¿®æ”¹åçš„é€»è¾‘) VVVV ---
Â  Â  Â  Â  Â  Â  if main_kw == "ä¿è¯é‡‘æ¯”ä¾‹":
Â  Â  Â  Â  Â  Â  Â  Â  num_errors = (diff > 0.00500001) # ä¿è¯é‡‘æ¯”ä¾‹å®¹é”™
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  elif "ç§ŸèµæœŸé™" in main_kw: # åŒ¹é… "ç§ŸèµæœŸé™" å’Œ "ç§ŸèµæœŸé™æœˆ"
Â  Â  Â  Â  Â  Â  Â  Â  # å¿½ç•¥å°äº 1.0 ä¸ªæœˆçš„å·®è· (å³ï¼šå·®å¼‚ >= 1.0 æ‰ç®—é”™è¯¯)
Â  Â  Â  Â  Â  Â  Â  Â  num_errors = (diff >= 1.0) 
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # å…¶ä»–æ•°å€¼å­—æ®µï¼Œä½¿ç”¨æ ‡å‡†å¾®å°å®¹é”™
Â  Â  Â  Â  Â  Â  Â  Â  num_errors = (diff > 1e-6)
Â  Â  Â  Â  Â  Â  # --- ^^^^ (ä¿®æ”¹ç»“æŸ) ^^^^ ---
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  errors.loc[both_are_num] = num_errors

        # 3b. æ–‡æœ¬æ¯”è¾ƒ
        not_num_mask = ~both_are_num
        if not_num_mask.any():
            str_main = s_main_norm[not_num_mask].astype(str).str.strip().str.replace(r"\.0$", "", regex=True)
            str_ref = s_ref_norm[not_num_mask].astype(str).str.strip().str.replace(r"\.0$", "", regex=True)
            
            str_errors = (str_main != str_ref)
            errors.loc[not_num_mask] = str_errors
            
        # æ’é™¤æ‰é‚£äº›ä¸¤è€…çš†ä¸ºç©ºçš„æƒ…å†µ
        errors = errors & ~both_are_na_norm
        # return errors # (è¿™æ˜¯æ—§çš„è¿”å›)

    # 4. === æœ€ç»ˆé”™è¯¯é€»è¾‘ ===
    
    # a. æ’é™¤ "ä¸¤è€…çš†ä¸ºç©º" çš„æƒ…å†µ (åŸå§‹é€»è¾‘)
    final_errors = errors & ~both_are_na
    
    # b. æ’é™¤ "Merge å¤±è´¥" çš„æƒ…å†µ (å¤åˆ» iterrows çš„ 'if ref_rows.empty: return 0')
    #    æ¡ä»¶:
    #    1. merge_failed_mask ä¸º True (s_ref æ˜¯ç‰©ç† NaN)
    #    2. main_is_na ä¸º False (s_main ä¸æ˜¯ç©ºçš„)
    #    å¦‚æœ (1) å’Œ (2) éƒ½æˆç«‹ï¼Œè¯´æ˜è¿™æ˜¯ä¸€ä¸ª "lookup failure"ï¼Œæˆ‘ä»¬å¿…é¡»å¿½ç•¥å®ƒ
    
    lookup_failure_mask = merge_failed_mask & ~main_is_na
    
    final_errors = final_errors & ~lookup_failure_mask
    
    return final_errors

# =====================================
# ğŸ§® å•sheetæ£€æŸ¥å‡½æ•° (å‘é‡åŒ–ç‰ˆ)
# =====================================
def check_one_sheet(sheet_keyword, main_file, ref_dfs_std_dict):
    start_time = time.time()
    xls_main = pd.ExcelFile(main_file)

    # æŸ¥æ‰¾ç›®æ ‡sheet
    try:
        target_sheet = find_sheet(xls_main, sheet_keyword)
    except ValueError:
        st.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ…å«ã€Œ{sheet_keyword}ã€çš„sheetï¼Œè·³è¿‡ã€‚")
        return 0, None, 0, set()

    # 1. è¯»å–ç›®æ ‡sheetï¼ˆç¬¬äºŒè¡Œä¸ºè¡¨å¤´ï¼‰
    try:
        main_df = pd.read_excel(xls_main, sheet_name=target_sheet, header=1)
    except Exception as e:
        st.error(f"âŒ è¯»å–ã€Œ{sheet_keyword}ã€æ—¶å‡ºé”™: {e}")
        return 0, None, 0, set()
        
    if main_df.empty:
        st.warning(f"âš ï¸ ã€Œ{sheet_keyword}ã€ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
        return 0, None, 0, set()

    # 2. æŸ¥æ‰¾åˆåŒå·åˆ—
    global contract_col_main
    contract_col_main = find_col(main_df, "åˆåŒ")
    if not contract_col_main:
        st.error(f"âŒ åœ¨ã€Œ{sheet_keyword}ã€ä¸­æœªæ‰¾åˆ°åˆåŒåˆ—ã€‚")
        return 0, None, 0, set()

    # 3. åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶ (ä¿ç•™åŸå§‹è¡¨å¤´ç©ºè¡Œ)
    output_path = f"æœˆé‡å¡_{sheet_keyword}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx"
    empty_row = pd.DataFrame([[""] * len(main_df.columns)], columns=main_df.columns)
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿å­˜çš„æ˜¯åŸå§‹main_df
    pd.concat([empty_row, main_df], ignore_index=True).to_excel(output_path, index=False)

    # æ‰“å¼€Excelç”¨äºå†™å…¥æ ‡æ³¨
    wb = load_workbook(output_path)
    ws = wb.active
    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

   # 4. å‡†å¤‡ä¸»è¡¨ç”¨äºåˆå¹¶
    # (æ³¨æ„ï¼šcontract_col_main å·²ç»åœ¨ç¬¬ 236 è¡Œè¢«æ­£ç¡®æ‰¾åˆ°äº†ï¼Œæˆ‘ä»¬ä¸éœ€è¦å†æ‰¾äº†)
    
    # å­˜å‚¨åŸå§‹ç´¢å¼•ï¼Œç”¨äº openpyxl å®šä½
    main_df['__ROW_IDX__'] = main_df.index
    
    # VVVV æ’å…¥å½’ä¸€åŒ–å‡½æ•° VVVV
    # åˆ›å»ºæ ‡å‡†åˆå¹¶Key
    main_df['__KEY__'] = normalize_contract_key(main_df[contract_col_main])
    # ^^^^ æ’å…¥å½’ä¸€åŒ–å‡½æ•° ^^^^
    
    # è·å–æœ¬è¡¨æ‰€æœ‰åˆåŒå·ï¼ˆç”¨äºç»Ÿè®¡ç­‰ï¼‰
    contracts_seen = set(main_df['__KEY__'].dropna())

    # 5. ä¸€æ¬¡æ€§åˆå¹¶æ‰€æœ‰å‚è€ƒæ•°æ®
    merged_df = main_df.copy()
    for prefix, std_df in ref_dfs_std_dict.items():
        if not std_df.empty:
            merged_df = pd.merge(merged_df, std_df, on='__KEY__', how='left')
    
    total_errors = 0
    skip_city_manager = [0]
    errors_locations = set() # å­˜å‚¨ (row_idx, col_name)
    row_has_error = pd.Series(False, index=merged_df.index) # æ ‡è®°å“ªä¸€è¡Œæœ‰é”™è¯¯

    # æ·»åŠ Streamlitè¿›åº¦æ¡
    progress = st.progress(0)
    status = st.empty()

    # 6. === éå†å­—æ®µè¿›è¡Œå‘é‡åŒ–æ¯”å¯¹ ===
    mappings_all = {
        'fk': (mapping_fk, ref_dfs_std_dict['fk']),
        'zd': (mapping_zd, ref_dfs_std_dict['zd']),
        'ec': (mapping_ec, ref_dfs_std_dict['ec'])
    }
    
    total_comparisons = sum(len(m[0]) for m in mappings_all.values())
    current_comparison = 0

    for prefix, (mapping, std_df) in mappings_all.items():
        if std_df.empty:
            current_comparison += len(mapping) # è·³è¿‡ç©ºè¡¨
            continue
            
        for main_kw, ref_kw in mapping.items():
            current_comparison += 1
            status.text(f"æ£€æŸ¥ã€Œ{sheet_keyword}ã€: {prefix} - {main_kw}...")
            
            # å…³é”®ï¼šåœ¨åŸå§‹ main_df ä¸­æ‰¾åˆ°åˆ—å
            exact = (main_kw == "åŸå¸‚ç»ç†")
            main_col = find_col(main_df, main_kw, exact=exact)
            
            # å‚è€ƒåˆ—çš„åˆ—åæ˜¯æˆ‘ä»¬åœ¨ prepare_ref_df ä¸­æ ‡å‡†åŒ–çš„
            ref_col = f'ref_{prefix}_{main_kw}'

            if not main_col or ref_col not in merged_df.columns:
                continue # è·³è¿‡ä¸å­˜åœ¨çš„åˆ—

            s_main = merged_df[main_col]
            s_ref = merged_df[ref_col]

            # å¤„ç† "åŸå¸‚ç»ç†" è·³è¿‡é€»è¾‘
            skip_mask = pd.Series(False, index=merged_df.index)
            if main_kw == "åŸå¸‚ç»ç†":
                na_strings = ["", "-", "nan", "none", "null"]
                # æ£€æŸ¥å‚è€ƒåˆ—æ˜¯å¦ä¸ºç©º
                skip_mask = pd.isna(s_ref) | s_ref.astype(str).str.strip().isin(na_strings)
                skip_city_manager[0] += skip_mask.sum()
            
            # 7. è·å–å‘é‡åŒ–æ¯”è¾ƒç»“æœ
            errors_mask = compare_series_vec(s_main, s_ref, main_kw)
            
            # åº”ç”¨è·³è¿‡é€»è¾‘ï¼šå¦‚æœ skip_mask ä¸º Trueï¼Œåˆ™ä¸ç®—é”™è¯¯
            final_errors_mask = errors_mask & ~skip_mask
            
            if final_errors_mask.any():
                total_errors += final_errors_mask.sum()
                row_has_error |= final_errors_mask
                
                # 8. å­˜å‚¨é”™è¯¯ä½ç½® (ä½¿ç”¨ __ROW_IDX__ å’Œ åŸå§‹ main_col åç§°)
                bad_indices = merged_df[final_errors_mask]['__ROW_IDX__']
                for idx in bad_indices:
                    errors_locations.add((idx, main_col))
                    
            progress.progress(current_comparison / total_comparisons)

    status.text(f"ã€Œ{sheet_keyword}ã€æ¯”å¯¹å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæ ‡æ³¨æ–‡ä»¶...")

    # 9. === éå†é”™è¯¯è¿›è¡ŒExcelæ ‡æ³¨ ===
    # (è¿™æ¯”éå†æ‰€æœ‰å•å…ƒæ ¼å¿«å¾—å¤š)
    
    # è·å–åŸå§‹åˆ—å (å»æ‰æˆ‘ä»¬æ·»åŠ çš„è¾…åŠ©åˆ—)
    original_cols_list = list(main_df.drop(columns=['__ROW_IDX__', '__KEY__']).columns)
    # åˆ›å»ºåˆ—ååˆ°Excelåˆ—ç´¢å¼•(1-based)çš„æ˜ å°„
    col_name_to_idx = {name: i + 1 for i, name in enumerate(original_cols_list)}

    # æ ‡çº¢é”™è¯¯å•å…ƒæ ¼
    for (row_idx, col_name) in errors_locations:
        if col_name in col_name_to_idx:
            # +3: (1-based index) + (1 for header) + (1 for empty row)
            ws.cell(row_idx + 3, col_name_to_idx[col_name]).fill = red_fill

    # æ ‡é»„æœ‰é”™è¯¯çš„åˆåŒå·
    if contract_col_main in col_name_to_idx:
        contract_col_excel_idx = col_name_to_idx[contract_col_main]
        # æ‰¾åˆ°æ‰€æœ‰å‡ºé”™çš„åŸå§‹è¡Œå·
        error_row_indices = merged_df[row_has_error]['__ROW_IDX__']
        for row_idx in error_row_indices:
            ws.cell(row_idx + 3, contract_col_excel_idx).fill = yellow_fill

    # 10. å¯¼å‡ºæ£€æŸ¥ç»“æœ
    output = BytesIO()
    wb.save(output)
    output.seek(0)
    st.download_button(
        label=f"ğŸ“¥ ä¸‹è½½ {sheet_keyword}å®¡æ ¸æ ‡æ³¨ç‰ˆ",
        data=output,
        file_name=f"è®°å½•è¡¨_{sheet_keyword}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx",
        key=f"download_{sheet_keyword}" # å¢åŠ keyé¿å…streamlité‡è·‘é—®é¢˜
    )

    # 11. (æ–°) å¯¼å‡ºä»…å«é”™è¯¯è¡Œçš„æ–‡ä»¶ (å¸¦æ ‡çº¢)
    if row_has_error.any():
        try:
            # 1. è·å–ä»…å«é”™è¯¯è¡Œçš„ DataFrame (åªä¿ç•™åŸå§‹åˆ—)
            #    (original_cols_list å·²åœ¨ç¬¬ 365 è¡Œå®šä¹‰)
            df_errors_only = merged_df.loc[row_has_error, original_cols_list].copy()
            
            # 2. å…³é”®ï¼šåˆ›å»º "åŸå§‹è¡Œç´¢å¼•" åˆ° "æ–°Excelè¡Œå·" çš„æ˜ å°„
            #    æˆ‘ä»¬è·å–æ‰€æœ‰å‡ºé”™è¡Œçš„ __ROW_IDX__
            original_indices_with_error = merged_df.loc[row_has_error, '__ROW_IDX__']
            
            #    åˆ›å»ºæ˜ å°„: { åŸå§‹ç´¢å¼• : æ–°çš„Excelè¡Œå· }
            #    (enumerate start=2, å› ä¸º Excel è¡Œ 1 æ˜¯è¡¨å¤´, æ•°æ®ä»è¡Œ 2 å¼€å§‹)
            original_idx_to_new_excel_row = {
                original_idx: new_row_num 
                for new_row_num, original_idx in enumerate(original_indices_with_error, start=2)
            }

            # 3. åˆ›å»ºä¸€ä¸ªæ–°çš„å·¥ä½œç°¿(Workbook)
            wb_errors = Workbook()
            ws_errors = wb_errors.active
            
            # 4. ä½¿ç”¨ dataframe_to_rows å¿«é€Ÿå†™å…¥æ•°æ®
            for r in dataframe_to_rows(df_errors_only, index=False, header=True):
                ws_errors.append(r)
                
            # 5. éå†ä¸»é”™è¯¯åˆ—è¡¨(errors_locations)ï¼Œè¿›è¡Œæ ‡çº¢
            #    (col_name_to_idx å’Œ red_fill å·²åœ¨å‰é¢å®šä¹‰)
            for (original_row_idx, col_name) in errors_locations:
                
                # æ£€æŸ¥è¿™ä¸ªé”™è¯¯æ˜¯å¦åœ¨æˆ‘ä»¬ "ä»…é”™è¯¯è¡Œ" çš„æ˜ å°„ä¸­
                if original_row_idx in original_idx_to_new_excel_row:
                    
                    # è·å–å®ƒåœ¨æ–°Excelæ–‡ä»¶ä¸­çš„è¡Œå·
                    new_row = original_idx_to_new_excel_row[original_row_idx]
                    
                    # è·å–åˆ—å·
                    if col_name in col_name_to_idx:
                        new_col = col_name_to_idx[col_name]
                        
                        # åº”ç”¨æ ‡çº¢
                        ws_errors.cell(row=new_row, column=new_col).fill = red_fill
            
            # 6. ä¿å­˜åˆ° BytesIO
            output_errors_only = BytesIO()
            wb_errors.save(output_errors_only)
            output_errors_only.seek(0)
            
            # 7. åˆ›å»ºä¸‹è½½æŒ‰é’®
            st.download_button(
                label=f"ğŸ“¥ ä¸‹è½½ {sheet_keyword} (ä»…å«é”™è¯¯è¡Œ, å¸¦æ ‡çº¢)", # æ›´æ–°äº†æ ‡ç­¾
                data=output_errors_only,
                file_name=f"è®°å½•è¡¨_{sheet_keyword}_ä»…é”™è¯¯è¡Œ_æ ‡çº¢.xlsx", # æ›´æ–°äº†æ–‡ä»¶å
                key=f"download_{sheet_keyword}_errors_only" # Key ä¿æŒä¸å˜
            )
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆâ€œä»…é”™è¯¯è¡Œâ€æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    elapsed = time.time() - start_time
    st.success(f"âœ… {sheet_keyword} æ£€æŸ¥å®Œæˆï¼Œå…± {total_errors} å¤„é”™è¯¯ï¼Œç”¨æ—¶ {elapsed:.2f} ç§’ã€‚")
    return total_errors, elapsed, skip_city_manager[0], contracts_seen

# =====================================
# ğŸ“– æ–‡ä»¶è¯»å–ï¼šæŒ‰å…³é”®å­—è¯†åˆ«äº”ä»½æ–‡ä»¶
# =====================================
main_file = find_file(uploaded_files, "æœˆé‡å¡")
fk_file = find_file(uploaded_files, "æ”¾æ¬¾æ˜ç»†")
zd_file = find_file(uploaded_files, "å­—æ®µ")
ec_file = find_file(uploaded_files, "äºŒæ¬¡æ˜ç»†")

# å„æ–‡ä»¶sheetè¯»å–ï¼ˆæ¨¡ç³ŠåŒ¹é…sheetåï¼‰
fk_df = pd.read_excel(pd.ExcelFile(fk_file), sheet_name=find_sheet(pd.ExcelFile(fk_file), "å¨ç”°"))
zd_df = pd.read_excel(pd.ExcelFile(zd_file), sheet_name=find_sheet(pd.ExcelFile(zd_file), "é‡å¡"))
ec_df = pd.read_excel(ec_file)

# åˆåŒåˆ—å®šä½
contract_col_fk = find_col(fk_df, "åˆåŒ")
contract_col_zd = find_col(zd_df, "åˆåŒ")
contract_col_ec = find_col(ec_df, "åˆåŒ")

# å¯¹ç…§å­—æ®µæ˜ å°„è¡¨
# --- VVVV (è¿™æ˜¯æ–°çš„ï¼Œä¿®æ­£çš„) VVVV ---
# æ ¼å¼: {"è®°å½•è¡¨(ä¸»è¡¨)çš„åˆ—å": "æ”¾æ¬¾æ˜ç»†(å‚è€ƒè¡¨)çš„åˆ—åå…³é”®å­—"}
mapping_fk = {
    # è¿™3ä¸ªæ˜¯æ­£ç¡®çš„
    "æˆä¿¡æ–¹": "æˆä¿¡æ–¹",     # "æˆä¿¡æ–¹" in "æˆä¿¡æ–¹"
    "ç§Ÿèµæœ¬é‡‘": "ç§Ÿèµæœ¬é‡‘",  
    "ç§ŸèµæœŸé™": "ç§ŸèµæœŸé™",
    
    # è¿™4ä¸ªæ˜¯ä¿®æ­£çš„
    "æŒ‚è½¦å°æ•°": "æŒ‚è½¦æ•°é‡",     # "æŒ‚è½¦æ•°é‡" in "æŒ‚è½¦æ•°é‡"
    "èµ·ç§Ÿæ”¶ç›Šç‡": "XIRR"      # å‡è®¾ "è´¹ç‡" æ˜¯æ‚¨æƒ³è¦çš„ "æ”¶ç›Šç‡"ã€‚å¦‚æœä¸æ˜¯ï¼Œè¯·ä¿®æ”¹ä¸º "XIRR" æˆ–å…¶ä»–
}
# --- ^^^^ (ä¿®æ­£ç»“æŸ) ^^^^ ---


mapping_zd = {"ä¿è¯é‡‘æ¯”ä¾‹": "ä¿è¯é‡‘æ¯”ä¾‹_2", "é¡¹ç›®ææŠ¥äºº": "ææŠ¥", "èµ·ç§Ÿæ—¶é—´": "èµ·ç§Ÿæ—¥_å•†", "å®¢æˆ·ç»ç†": "å®¢æˆ·ç»ç†_èµ„äº§", "æ‰€å±çœåŒº": "åŒºåŸŸ", "ä¸»è½¦å°æ•°": "ä¸»è½¦å°æ•°", "åŸå¸‚ç»ç†": "åŸå¸‚ç»ç†"}
mapping_ec = {"äºŒæ¬¡æ—¶é—´": "å‡ºæœ¬æµç¨‹æ—¶é—´"}

# =====================================
# ğŸš€ (æ–°) é¢„å¤„ç†æ‰€æœ‰å‚è€ƒè¡¨
# =====================================
st.info("â„¹ï¸ æ­£åœ¨é¢„å¤„ç†å‚è€ƒæ•°æ®...")

# (fk_df, zd_df, ec_df, zk_df å¿…é¡»å·²ç»åŠ è½½)
fk_std = prepare_ref_df(fk_df, mapping_fk, 'fk')
zd_std = prepare_ref_df(zd_df, mapping_zd, 'zd')
ec_std = prepare_ref_df(ec_df, mapping_ec, 'ec')

# å°†æ‰€æœ‰é¢„å¤„ç†è¿‡çš„DFå­˜å…¥å­—å…¸ï¼Œä¼ é€’ç»™æ£€æŸ¥å‡½æ•°
ref_dfs_std_dict = {
    'fk': fk_std,
    'zd': zd_std,
    'ec': ec_std
}
st.success("âœ… å‚è€ƒæ•°æ®é¢„å¤„ç†å®Œæˆã€‚")

# =====================================
# ğŸ§¾ å¤šsheetå¾ªç¯ + é©»åº—å®¢æˆ·è¡¨
# =====================================
sheet_keywords = ["äºŒæ¬¡", "éƒ¨åˆ†æ‹…ä¿", "éšå·", "é©»åº—å®¢æˆ·"]
total_all = elapsed_all = skip_total = 0
contracts_seen_all_sheets = set()

# å¾ªç¯å¤„ç†å››å¼ sheet (è°ƒç”¨æ–°å‡½æ•°)
for kw in sheet_keywords:
    # å°† main_file å’Œ ref_dfs_std_dict ä¼ é€’è¿›å»
    count, used, skipped, seen = check_one_sheet(kw, main_file, ref_dfs_std_dict)
    
    total_all += count
    elapsed_all += used or 0
    skip_total += skipped
    contracts_seen_all_sheets.update(seen)

st.success(f"ğŸ¯ å…¨éƒ¨å®¡æ ¸å®Œæˆï¼Œå…± {total_all} å¤„é”™è¯¯ï¼Œæ€»è€—æ—¶ {elapsed_all:.2f} ç§’ã€‚")

# =====================================
# ğŸ•µï¸ æ¼å¡«æ£€æŸ¥ï¼šè·³è¿‡â€œæ˜¯å¦è½¦ç®¡å®¶=æ˜¯â€ä¸â€œææˆç±»å‹=è”åˆç§Ÿèµ/é©»åº—â€
# =====================================
field_contracts = zd_df[contract_col_zd].dropna().astype(str).str.strip()
col_car_manager = find_col(zd_df, "æ˜¯å¦è½¦ç®¡å®¶", exact=True)
col_bonus_type = find_col(zd_df, "ææˆç±»å‹", exact=True)

missing_contracts_mask = (~field_contracts.isin(contracts_seen_all_sheets))

# è·³è¿‡â€œè½¦ç®¡å®¶=æ˜¯â€
if col_car_manager:
    missing_contracts_mask &= ~(zd_df[col_car_manager].astype(str).str.strip().str.lower() == "æ˜¯")
# è·³è¿‡â€œè”åˆç§Ÿèµ/é©»åº—â€
if col_bonus_type:
    missing_contracts_mask &= ~(
        zd_df[col_bonus_type].astype(str).str.strip().isin(["è”åˆç§Ÿèµ", "é©»åº—"])
    )

# æ ‡è®°æ¼å¡«
zd_df_missing = zd_df.copy()
zd_df_missing["æ¼å¡«æ£€æŸ¥"] = ""
zd_df_missing.loc[missing_contracts_mask, "æ¼å¡«æ£€æŸ¥"] = "â— æ¼å¡«"
æ¼å¡«åˆåŒæ•° = zd_df_missing["æ¼å¡«æ£€æŸ¥"].eq("â— æ¼å¡«").sum()
st.warning(f"âš ï¸ å…±å‘ç° {æ¼å¡«åˆåŒæ•°} ä¸ªåˆåŒåœ¨è®°å½•è¡¨ä¸­æœªå‡ºç°ï¼ˆå·²æ’é™¤è½¦ç®¡å®¶ã€è”åˆç§Ÿèµã€é©»åº—ï¼‰")

# =====================================
# ğŸ“¤ å¯¼å‡ºå­—æ®µè¡¨ï¼ˆå«æ¼å¡«æ ‡æ³¨ + ä»…æ¼å¡«ç‰ˆï¼‰
# =====================================
yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

# å…¨å­—æ®µè¡¨ï¼ˆå«æ¼å¡«æ ‡æ³¨ï¼‰
wb = Workbook()
ws = wb.active
for c_idx, c in enumerate(zd_df_missing.columns, 1): ws.cell(1, c_idx, c)
for r_idx, row in enumerate(zd_df_missing.itertuples(index=False), 2):
    for c_idx, v in enumerate(row, 1):
        ws.cell(r_idx, c_idx, v)
        if zd_df_missing.columns[c_idx-1] == "æ¼å¡«æ£€æŸ¥" and v == "â— æ¼å¡«":
            ws.cell(r_idx, c_idx).fill = yellow_fill

output_all = BytesIO()
wb.save(output_all)
output_all.seek(0)
st.download_button("ğŸ“¥ ä¸‹è½½å­—æ®µè¡¨æ¼å¡«æ ‡æ³¨ç‰ˆ", output_all, "å­—æ®µè¡¨_æ¼å¡«æ ‡æ³¨ç‰ˆ.xlsx")

# ä»…æ¼å¡«åˆåŒ
zd_df_only_missing = zd_df_missing[zd_df_missing["æ¼å¡«æ£€æŸ¥"] == "â— æ¼å¡«"].copy()
if not zd_df_only_missing.empty:
    wb2 = Workbook()
    ws2 = wb2.active
    for c_idx, c in enumerate(zd_df_only_missing.columns, 1): ws2.cell(1, c_idx, c)
    for r_idx, row in enumerate(zd_df_only_missing.itertuples(index=False), 2):
        for c_idx, v in enumerate(row, 1):
            ws2.cell(r_idx, c_idx, v)
            if zd_df_only_missing.columns[c_idx-1] == "æ¼å¡«æ£€æŸ¥" and v == "â— æ¼å¡«":
                ws2.cell(r_idx, c_idx).fill = yellow_fill
    out2 = BytesIO()
    wb2.save(out2)
    out2.seek(0)
    st.download_button("ğŸ“¥ ä¸‹è½½ä»…æ¼å¡«å­—æ®µè¡¨", out2, "å­—æ®µè¡¨_ä»…æ¼å¡«.xlsx")

# =====================================
# âœ… ç»“æŸæç¤º
# =====================================
st.success("âœ… æ‰€æœ‰æ£€æŸ¥ã€æ ‡æ³¨ä¸å¯¼å‡ºå®Œæˆï¼")
