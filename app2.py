# =====================================
# Streamlit Web App: æ¨¡æ‹ŸProjectâ€”â€”äººäº‹ç”¨åˆåŒè®°å½•è¡¨è‡ªåŠ¨å®¡æ ¸ç³»ç»Ÿ
# ï¼ˆå››Sheet + å‘é‡åŒ–ä¼˜åŒ– + æ¼å¡«æ£€æŸ¥ + é©»åº—å®¢æˆ·ï¼‰
# =====================================

import streamlit as st
import pandas as pd
import time
from openpyxl import Workbook
from openpyxl.styles import PatternFill
from io import BytesIO

st.title("ğŸ“Š æ¨¡æ‹ŸProjectï¼šäººäº‹ç”¨åˆåŒè®°å½•è¡¨è‡ªåŠ¨å®¡æ ¸ç³»ç»Ÿï¼ˆå››Sheet + å‘é‡åŒ–ä¼˜åŒ– + æ¼å¡«æ£€æŸ¥ + é©»åº—å®¢æˆ·ï¼‰")

# -------- ä¸Šä¼ æ–‡ä»¶ ----------
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼ ä»¥ä¸‹æ–‡ä»¶ï¼šè®°å½•è¡¨ã€æ”¾æ¬¾æ˜ç»†ã€å­—æ®µã€äºŒæ¬¡æ˜ç»†ã€é‡å¡æ•°æ®",
    type="xlsx",
    accept_multiple_files=True
)

if not uploaded_files or len(uploaded_files) < 5:
    st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ 5 ä¸ªæ–‡ä»¶åç»§ç»­")
    st.stop()
else:
    st.success("âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ")

# -------- å·¥å…·ï¼šæŸ¥æ‰¾æ–‡ä»¶ / åˆ— / sheet ----------
def find_file(files_list, keyword):
    for f in files_list:
        if keyword in f.name:
            return f
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„æ–‡ä»¶")

def normalize_colname(c):
    return str(c).strip().lower()

def find_col(df, keyword, exact=False):
    if df is None:
        return None
    key = keyword.strip().lower()
    for col in df.columns:
        cname = normalize_colname(col)
        if (exact and cname == key) or (not exact and key in cname):
            return col
    return None

def find_sheet(xls, keyword):
    for s in xls.sheet_names:
        if keyword in s:
            return s
    raise ValueError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„sheet")

# å°çš„å¸®åŠ©ï¼šæŠŠåˆåŒåˆ—ç»Ÿä¸€ä¸º string ä¸” strip
def normalize_contract_col(df, col):
    if col is None:
        return df
    df[col] = df[col].astype(str).where(~df[col].isna(), "").str.strip()
    return df

# æ—¥æœŸæŒ‰å¹´æœˆæ—¥åˆ¤æ–­
def ymd_series(s):
    return pd.to_datetime(s, errors="coerce").dt.normalize()

# -------- è¯»å–æ–‡ä»¶å¹¶å‡†å¤‡ dataframes ----------
main_file = find_file(uploaded_files, "è®°å½•è¡¨")
fk_file   = find_file(uploaded_files, "æ”¾æ¬¾æ˜ç»†")
zd_file   = find_file(uploaded_files, "å­—æ®µ")
ec_file   = find_file(uploaded_files, "äºŒæ¬¡æ˜ç»†")
zk_file   = find_file(uploaded_files, "é‡å¡æ•°æ®")

fk_xls = pd.ExcelFile(fk_file)
zd_xls = pd.ExcelFile(zd_file)

# è¯»å–å‚ç…§è¡¨ï¼ˆç”¨æ¨¡ç³ŠåŒ¹é… sheet åï¼‰
fk_df = pd.read_excel(fk_xls, sheet_name=find_sheet(fk_xls, "æœ¬å¸"))
zd_df = pd.read_excel(zd_xls, sheet_name=find_sheet(zd_xls, "é‡å¡"))
ec_df = pd.read_excel(ec_file)
zk_df = pd.read_excel(zk_file)

# é˜²æ­¢å‚ç…§è¡¨åˆ—åé‡å¤å¯¼è‡´ merge æŠ¥é”™ï¼šå¯¹æ¯ä¸ªå‚ç…§è¡¨ä¿ç•™ç¬¬ä¸€ç»„åŒååˆ—
def dedup_keep_first(df):
    # å¦‚æœå­˜åœ¨é‡å¤åˆ—åï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°ï¼Œåˆ é™¤é‡å¤åçš„åŒååˆ—
    cols = df.columns.tolist()
    seen = set()
    keep_cols = []
    for c in cols:
        if c not in seen:
            keep_cols.append(c)
            seen.add(c)
    return df.loc[:, keep_cols]

fk_df = dedup_keep_first(fk_df)
zd_df = dedup_keep_first(zd_df)
ec_df = dedup_keep_first(ec_df)
zk_df = dedup_keep_first(zk_df)

# -------- æ˜ å°„é…ç½®ï¼ˆä¿æŒä¸ä½ ä¹‹å‰ç›¸åŒï¼‰ ----------
mapping_fk = {
    "æˆä¿¡æ–¹": "æˆä¿¡",
    "ç§Ÿèµæœ¬é‡‘": "æœ¬é‡‘",
    "ç§ŸèµæœŸé™æœˆ": "ç§ŸèµæœŸé™æœˆ",
    "å®¢æˆ·ç»ç†": "å®¢æˆ·ç»ç†",
    "èµ·ç§Ÿæ”¶ç›Šç‡": "æ”¶ç›Šç‡",
    "ä¸»è½¦å°æ•°": "ä¸»è½¦å°æ•°",
    "æŒ‚è½¦å°æ•°": "æŒ‚è½¦å°æ•°"
}
mapping_zd = {
    "ä¿è¯é‡‘æ¯”ä¾‹": "ä¿è¯é‡‘æ¯”ä¾‹_2",
    "é¡¹ç›®ææŠ¥äºº": "ææŠ¥",
    "èµ·ç§Ÿæ—¶é—´": "èµ·ç§Ÿæ—¥_å•†",
    "ç§ŸèµæœŸé™æœˆ": "æ€»æœŸæ•°_å•†_èµ„äº§",
    "æ‰€å±çœåŒº": "åŒºåŸŸ",
    "åŸå¸‚ç»ç†": "åŸå¸‚ç»ç†"
}
mapping_ec = {"äºŒæ¬¡æ—¶é—´": "å‡ºæœ¬æµç¨‹æ—¶é—´"}
mapping_zk = {"æˆä¿¡æ–¹": "æˆä¿¡æ–¹"}

# -------- è¯†åˆ«å‚ç…§è¡¨åˆåŒåˆ—ï¼ˆä¼šåœ¨æ¯æ¬¡è¿è¡Œå‰normalizeåˆåŒåˆ—ï¼‰ ----------
contract_col_fk = find_col(fk_df, "åˆåŒ")
contract_col_zd = find_col(zd_df, "åˆåŒ")
contract_col_ec = find_col(ec_df, "åˆåŒ")
contract_col_zk = find_col(zk_df, "åˆåŒ")

# æŠŠå‚ç…§è¡¨çš„åˆåŒåˆ—ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²å¹¶ stripï¼ˆæ–¹ä¾¿ later merge/isinï¼‰
if contract_col_fk:
    fk_df = normalize_contract_col(fk_df, contract_col_fk)
if contract_col_zd:
    zd_df = normalize_contract_col(zd_df, contract_col_zd)
if contract_col_ec:
    ec_df = normalize_contract_col(ec_df, contract_col_ec)
if contract_col_zk:
    zk_df = normalize_contract_col(zk_df, contract_col_zk)

# -------- æ‰¹é‡/å‘é‡åŒ–æ¯”å¯¹è¾…åŠ©å‡½æ•° ----------
def safe_get_ref_col_name(merged_df, ref_kw):
    """merge åå¯èƒ½å‡ºç° ref_kw æˆ– ref_kw + '_ref'ï¼Œä¼˜å…ˆè¿”å›å­˜åœ¨çš„é‚£ä¸ª"""
    if ref_kw in merged_df.columns:
        return ref_kw
    if f"{ref_kw}_ref" in merged_df.columns:
        return f"{ref_kw}_ref"
    # as fallback, try any column that endswith ref_kw (rare)
    for c in merged_df.columns:
        if c.endswith(ref_kw):
            return c
    return None

def write_workbook_with_marks(main_df, red_marks_by_col, filename, contract_col_main, yellow_fill, red_fill):
    """
    main_df: åŸå§‹ä¸»è¡¨ï¼ˆæœªæ’å…¥ç©ºè¡Œï¼‰
    red_marks_by_col: dict main_col -> set(orig_idx) è¦æ ‡çº¢çš„è¡Œç´¢å¼•ï¼ˆè¿™äº›ç´¢å¼•å¯¹åº” main_df.indexï¼‰
    ç”Ÿæˆ workbook å¹¶è¿”å› BytesIO
    """
    wb = Workbook()
    ws = wb.active

    # å†™è¡¨å¤´
    for c_idx, c in enumerate(main_df.columns, start=1):
        ws.cell(1, c_idx, c)

    # å†™æ•°æ®è¡Œ
    for r_idx, (_, row) in enumerate(main_df.iterrows(), start=2):
        for c_idx, c in enumerate(main_df.columns, start=1):
            ws.cell(r_idx, c_idx, row[c])

    # æ ‡çº¢æŒ‡å®šå•å…ƒæ ¼ï¼ˆred_marks_by_colï¼‰
    for col_name, idx_set in red_marks_by_col.items():
        if col_name not in main_df.columns:
            continue
        col_idx = list(main_df.columns).index(col_name) + 1
        for orig_idx in idx_set:
            # æ‰¾åˆ°åœ¨ excel ä¸­å¯¹åº”çš„è¡Œå·ï¼ˆheader + 1blankè¡Œæœªè¦æ±‚ï¼Œè¿™é‡Œç›´æ¥å†™ main_dfï¼‰
            excel_row = list(main_df.index).index(orig_idx) + 2
            ws.cell(excel_row, col_idx).fill = red_fill

    # æ ‡é»„è‰²åˆåŒåˆ—ï¼ˆæ•´è¡Œæœ‰ä»»ä½•çº¢å°±é»„ï¼‰
    if contract_col_main in main_df.columns:
        contract_col_idx = list(main_df.columns).index(contract_col_main) + 1
        for r_idx, orig_idx in enumerate(main_df.index, start=2):
            # check if any red in that row: scan red_marks_by_col
            has_red = False
            for col_name, idx_set in red_marks_by_col.items():
                if orig_idx in idx_set:
                    has_red = True
                    break
            if has_red:
                ws.cell(r_idx, contract_col_idx).fill = yellow_fill

    bio = BytesIO()
    wb.save(bio)
    bio.seek(0)
    return bio

# -------- ä¸»æ£€æŸ¥å‡½æ•°ï¼ˆå‘é‡åŒ–ï¼‰ ----------
def check_one_sheet_fast(sheet_keyword):
    """å¯¹ä¸€ä¸ª sheet åšå‘é‡åŒ–æ£€æŸ¥ï¼Œè¿”å› (errors_count, elapsed_seconds, skip_city_manager_count, contracts_seen_set, excel_bytesio)"""
    start_time = time.time()
    xls_main = pd.ExcelFile(main_file)
    try:
        sheet_name = find_sheet(xls_main, sheet_keyword)
    except ValueError:
        st.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ…å«ã€Œ{sheet_keyword}ã€çš„sheetï¼Œè·³è¿‡ã€‚")
        return 0, 0, 0, set(), None

    main_df = pd.read_excel(xls_main, sheet_name=sheet_name, header=1)
    # ç»Ÿä¸€åˆåŒåˆ—å¹¶ä¿ç•™åŸå§‹ index ä¾¿äºå›å†™
    contract_col_main = find_col(main_df, "åˆåŒ")
    if not contract_col_main:
        st.error(f"âŒ åœ¨ {sheet_keyword} æœªæ‰¾åˆ°åˆåŒå·åˆ—")
        return 0, 0, 0, set(), None
    main_df = normalize_contract_col(main_df, contract_col_main)
    main_df["_orig_idx"] = main_df.index  # ä¿å­˜åŸå§‹ç´¢å¼•

    # red marks accumulator: main_col -> set(orig_idx)
    red_marks = {col: set() for col in main_df.columns}

    total_errors = 0
    skip_city_manager = 0
    contracts_seen = set(main_df[contract_col_main].dropna().astype(str).str.strip())

    # è¿›åº¦æ˜¾ç¤ºï¼ˆç²—ç•¥ï¼‰
    progress = st.progress(0)
    status = st.empty()

    # å†…éƒ¨ï¼šæ‰§è¡Œä¸€æ¬¡ mapping çš„æ‰¹é‡æ¯”è¾ƒ
    def batch_compare(mapping_dict, ref_df, ref_contract_col):
        nonlocal total_errors, skip_city_manager

        if ref_df is None or ref_contract_col is None:
            return

        # åªä¿ç•™å‚ç…§è¡¨éœ€è¦çš„åˆ—ï¼šåˆåŒåˆ— + æ˜ å°„ä¸­çš„å‚ç…§åˆ—ï¼ˆå­˜åœ¨åˆ™ä¿ç•™ï¼‰
        needed_ref_cols = []
        for v in mapping_dict.values():
            if v in ref_df.columns:
                needed_ref_cols.append(v)
        needed_ref_cols = [ref_contract_col] + needed_ref_cols
        ref_sub = ref_df.loc[:, [c for c in needed_ref_cols if c in ref_df.columns]].copy()

        # è§„èŒƒå‚ç…§è¡¨åˆåŒåˆ—ä¸ºå­—ç¬¦ä¸²
        if ref_contract_col in ref_sub.columns:
            ref_sub[ref_contract_col] = ref_sub[ref_contract_col].astype(str).where(~ref_sub[ref_contract_col].isna(), "").str.strip()

        # ä¸ºé¿å… merge æ—¶åˆ—åå†²çªï¼Œå…ˆæŠŠ ref_sub çš„åˆ—ä¸ main_df çš„åˆ—åæ¯”è¾ƒï¼›æˆ‘ä»¬ä¼šåœ¨ merge åä½¿ç”¨ safe_get_ref_col_name æŸ¥æ‰¾
        # æŠŠ main_df çš„åˆåŒå’Œ ref_sub çš„åˆåŒéƒ½ä½œä¸º key åšå·¦è¿æ¥
        merged = main_df.merge(
            ref_sub,
            left_on=contract_col_main,
            right_on=ref_contract_col,
            how="left",
            suffixes=("", "_ref")
        )

        # merged ä¸­ä¿ç•™åŸå§‹ç´¢å¼•åˆ— _orig_idx
        for main_kw, ref_kw in mapping_dict.items():
            # å¦‚æœä¸»è¡¨ä¸åŒ…å« main_kwï¼Œå°±è·³è¿‡ï¼ˆæ¯”å¦‚ç”¨æˆ·ä¸»è¡¨åˆ—åä¸å®Œå…¨ä¸€è‡´ï¼‰
            if main_kw not in merged.columns:
                continue

            # æ‰¾åˆ°å‚ç…§åˆ—åœ¨ merged ä¸­çš„åå­—ï¼ˆå¯èƒ½æ˜¯ ref_kw æˆ– ref_kw + '_ref'ï¼‰
            ref_col_in_merged = safe_get_ref_col_name(merged, ref_kw)
            if ref_col_in_merged is None:
                # å‚ç…§è¡¨ä¸å«è¯¥åˆ—ï¼Œè·³è¿‡
                continue

            a = merged[main_kw]            # æ¥è‡ªä¸»è¡¨çš„å€¼ï¼ˆSeriesï¼‰
            b = merged[ref_col_in_merged]  # æ¥è‡ªå‚ç…§è¡¨çš„å€¼ï¼ˆSeriesï¼‰

            # åŸå¸‚ç»ç†ï¼šå‚ç…§è¡¨ä¸ºç©ºçš„ç›´æ¥è·³è¿‡å¹¶è®¡æ•°ï¼ˆè§†ä¸ºæœªå¡«å†™ -> ä¸åˆ¤é”™ï¼‰
            if main_kw == "åŸå¸‚ç»ç†":
                # count rows where b is blank/NaN/empty string
                b_is_blank = b.isna() | (b.astype(str).str.strip() == "") | (b.astype(str).str.strip().str.lower().isin(["nan", "none", "null", "-"]))
                skip_city_manager += int(b_is_blank.sum())
                # for comparison, fill those b blanks with the a value so they won't be flagged below
                b = b.mask(b_is_blank, a)

            # åˆ¤æ–­æ˜¯å¦ä¸ºæ—¥æœŸå­—æ®µï¼ˆä¸»æˆ–å‚ç…§å­—æ®µååŒ…å«â€œæ—¥æœŸâ€æˆ–â€œæ—¶é—´â€ï¼‰
            is_date_field = any(k in main_kw for k in ["æ—¥æœŸ", "æ—¶é—´"]) or any(k in ref_kw for k in ["æ—¥æœŸ", "æ—¶é—´"])

            if is_date_field:
                # è½¬ä¸ºæ—¥æœŸï¼Œç„¶åæ¯”è¾ƒå¹´æœˆæ—¥æ˜¯å¦ä¸€è‡´
                a_dt = pd.to_datetime(a, errors="coerce")
                b_dt = pd.to_datetime(b, errors="coerce")
                a_na = a_dt.isna()
                b_na = b_dt.isna()
                both_na = a_na & b_na
                mismatch = ~( (a_dt.dt.year == b_dt.dt.year) & (a_dt.dt.month == b_dt.dt.month) & (a_dt.dt.day == b_dt.dt.day) )
                # å½“ä¸¤è€…éƒ½ä¸º NaT æ—¶ï¼Œä¸è§†ä¸º mismatch
                mismatch = mismatch & (~both_na)
            else:
                # å°è¯•æ•°å€¼æ¯”è¾ƒï¼ˆå‘é‡åŒ–ï¼‰
                a_num = pd.to_numeric(a, errors="coerce")
                b_num = pd.to_numeric(b, errors="coerce")
                a_na = a.isna()
                b_na = b.isna()
                both_na = a_na & b_na

                both_num = a_num.notna() & b_num.notna()
                # numeric mismatch
                # é’ˆå¯¹ä¿è¯é‡‘æ¯”ä¾‹ä½¿ç”¨å®¹å·® 0.005ï¼›æ³¨æ„ mapping_zd çš„é‚£ä¸€å¯¹
                if main_kw == "ä¿è¯é‡‘æ¯”ä¾‹" and ref_kw == "ä¿è¯é‡‘æ¯”ä¾‹_2":
                    tol = 0.005
                else:
                    tol = 1e-6

                numeric_mismatch = both_num & ( (a_num - b_num).abs() > tol )

                # éæ•°å€¼æ¯”è¾ƒï¼ˆå­—ç¬¦ä¸²æ¯”è¾ƒï¼‰ï¼šåœ¨æ­¤ä¹‹å‰æŠŠ NaN è§†ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä½†å¦‚æœä¸¤è€…å‡ä¸ºç©ºï¼Œåº”è§†ä¸ºç›¸ç­‰ï¼ˆå› æ­¤ç”¨ both_na å±è”½ï¼‰
                a_str = a.astype(str).where(~a_na, "").str.strip().str.lower()
                b_str = b.astype(str).where(~b_na, "").str.strip().str.lower()
                nonnum_mismatch = (~both_num) & (~both_na) & (a_str != b_str)

                mismatch = numeric_mismatch | nonnum_mismatch

            # æŠŠ mismatch è½¬ä¸ºéœ€è¦æ ‡çº¢çš„ä¸»è¡¨åŸå§‹ index é›†åˆ
            mismatch_idx = merged.loc[mismatch, "_orig_idx"].tolist()
            if mismatch_idx:
                total_errors += len(mismatch_idx)
                # accumulate
                red_marks.setdefault(main_kw, set()).update(mismatch_idx)

    # é€ä¸ª mapping æ‰¹é‡æ¯”è¾ƒï¼ˆå‘é‡åŒ–ï¼‰
    contract_col_fk_local = contract_col_fk
    contract_col_zd_local = contract_col_zd
    contract_col_ec_local = contract_col_ec
    contract_col_zk_local = contract_col_zk

    # æ‰§è¡Œæ‰¹é‡æ¯”è¾ƒï¼ˆæ¯æ¬¡éƒ½ä¼šåšä¸€æ¬¡ mergeï¼‰
    batch_compare(mapping_fk, fk_df, contract_col_fk_local)
    # æ˜ å°„å­—æ®µè¡¨
    batch_compare(mapping_zd, zd_df, contract_col_zd_local)
    # äºŒæ¬¡
    batch_compare(mapping_ec, ec_df, contract_col_ec_local)
    # é‡å¡
    batch_compare(mapping_zk, zk_df, contract_col_zk_local)

    # å†™å‡º Excelï¼ˆæŠŠ main_df åŸæ ·å†™å…¥ï¼Œç„¶ååº”ç”¨ red_marksï¼‰
    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

    # ç”Ÿæˆ workbook bytes
    bio = write_workbook_with_marks(main_df.drop(columns=["_orig_idx"]), red_marks, f"è®°å½•è¡¨_{sheet_keyword}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx", contract_col_main, yellow_fill, red_fill)

    elapsed = time.time() - start_time
    st.success(f"âœ… {sheet_keyword} æ£€æŸ¥å®Œæˆï¼Œå…± {total_errors} å¤„é”™è¯¯ï¼Œç”¨æ—¶ {elapsed:.2f} ç§’ã€‚")
    return total_errors, elapsed, skip_city_manager, contracts_seen, bio

# -------- å¾ªç¯æ‰§è¡Œå››ä¸ª sheet ----------
sheet_keywords = ["äºŒæ¬¡", "éƒ¨åˆ†æ‹…ä¿", "éšå·", "é©»åº—å®¢æˆ·"]
total_all = elapsed_all = skip_total = 0
contracts_seen_all_sheets = set()

# ä¸ºæ¯ä¸ª sheet æä¾›ä¸‹è½½æŒ‰é’®ï¼ˆå‘é‡åŒ–ç‰ˆæœ¬ç”Ÿæˆçš„ bytesioï¼‰
sheet_bios = {}

for kw in sheet_keywords:
    count, used, skipped, seen, bio = check_one_sheet_fast(kw)
    total_all += count
    elapsed_all += used
    skip_total += skipped
    contracts_seen_all_sheets.update(seen)
    sheet_bios[kw] = bio
    # æ˜¾ç¤ºå¹¶æä¾›ä¸‹è½½ï¼ˆå¦‚æœ bio ä¸º None åˆ™è·³è¿‡ï¼‰
    if bio is not None:
        st.download_button(label=f"ğŸ“¥ ä¸‹è½½ {kw} å®¡æ ¸æ ‡æ³¨ç‰ˆ", data=bio, file_name=f"è®°å½•è¡¨_{kw}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx")

st.success(f"ğŸ¯ å…¨éƒ¨æ£€æŸ¥å®Œæˆï¼Œå…± {total_all} å¤„é”™è¯¯ï¼Œæ€»è€—æ—¶ {elapsed_all:.2f} ç§’ã€‚")
st.info(f"ğŸ“ è·³è¿‡å­—æ®µè¡¨ä¸­ç©ºåŸå¸‚ç»ç†çš„åˆåŒæ•°é‡æ€»æ•°ï¼ˆä¼°ç®—ï¼‰ï¼š{skip_total}")

# -------- å­—æ®µè¡¨ æ¼å¡« æ£€æŸ¥ï¼ˆè·³è¿‡æ¡ä»¶ï¼šè½¦ç®¡å®¶=æ˜¯ï¼›ææˆç±»å‹=è”åˆç§Ÿèµ/é©»åº—ï¼‰ ----------
contract_col_zd = find_col(zd_df, "åˆåŒ")
col_car_manager = find_col(zd_df, "æ˜¯å¦è½¦ç®¡å®¶", exact=True)
col_bonus_type = find_col(zd_df, "ææˆç±»å‹", exact=True)

field_contracts = zd_df[contract_col_zd].dropna().astype(str).str.strip()
missing_mask = ~field_contracts.isin(contracts_seen_all_sheets)

# è·³è¿‡è½¦ç®¡å®¶=æ˜¯
if col_car_manager:
    missing_mask &= ~(zd_df[col_car_manager].astype(str).str.strip().str.lower() == "æ˜¯")
# è·³è¿‡ææˆç±»å‹è”åˆç§Ÿèµ/é©»åº—
if col_bonus_type:
    missing_mask &= ~(zd_df[col_bonus_type].astype(str).str.strip().isin(["è”åˆç§Ÿèµ", "é©»åº—"]))

zd_df_missing = zd_df.copy()
zd_df_missing["æ¼å¡«æ£€æŸ¥"] = ""
zd_df_missing.loc[missing_mask, "æ¼å¡«æ£€æŸ¥"] = "â— æ¼å¡«"

æ¼å¡«æ•° = zd_df_missing["æ¼å¡«æ£€æŸ¥"].eq("â— æ¼å¡«").sum()
st.warning(f"âš ï¸ å…±å‘ç° {æ¼å¡«æ•°} ä¸ªæ¼å¡«åˆåŒï¼ˆå·²æ’é™¤è½¦ç®¡å®¶ã€è”åˆç§Ÿèµã€é©»åº—ï¼‰")

# è¾“å‡ºå­—æ®µè¡¨ï¼ˆå¸¦é»„è‰²æ ‡æ³¨ï¼‰
def export_excel_with_yellow(df, filename):
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
    wb = Workbook()
    ws = wb.active
    # headers
    for c_idx, c in enumerate(df.columns, start=1):
        ws.cell(1, c_idx, c)
    # rows
    for r_idx, row in enumerate(df.itertuples(index=False), start=2):
        for c_idx, v in enumerate(row, start=1):
            ws.cell(r_idx, c_idx, v)
            if df.columns[c_idx-1] == "æ¼å¡«æ£€æŸ¥" and v == "â— æ¼å¡«":
                ws.cell(r_idx, c_idx).fill = yellow_fill
    bio = BytesIO()
    wb.save(bio)
    bio.seek(0)
    st.download_button(label=f"ğŸ“¥ ä¸‹è½½ {filename}", data=bio, file_name=f"{filename}.xlsx")

export_excel_with_yellow(zd_df_missing, "å­—æ®µè¡¨_æ¼å¡«æ ‡æ³¨ç‰ˆ")
zd_df_only_missing = zd_df_missing[zd_df_missing["æ¼å¡«æ£€æŸ¥"] == "â— æ¼å¡«"].copy()
if not zd_df_only_missing.empty:
    export_excel_with_yellow(zd_df_only_missing, "å­—æ®µè¡¨_ä»…æ¼å¡«")

st.success("âœ… æ‰€æœ‰æ£€æŸ¥ã€æ ‡æ³¨ä¸å¯¼å‡ºå®Œæˆï¼")

