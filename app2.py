# =====================================
# Streamlit Web App: æ¨¡æ‹ŸProjectï¼šäººäº‹ç”¨åˆåŒè®°å½•è¡¨è‡ªåŠ¨å®¡æ ¸ï¼ˆå››è¾“å‡ºè¡¨ç‰ˆ + æ¼å¡«æ£€æŸ¥ + é©»åº—å®¢æˆ·ç‰ˆï¼‰- âš¡å‘é‡åŒ–ä¼˜åŒ–ç‰ˆ
# =====================================

import streamlit as st
import pandas as pd
import numpy as np
import time
from openpyxl import Workbook
from openpyxl.styles import PatternFill
from io import BytesIO

# =====================================
# ğŸ åº”ç”¨æ ‡é¢˜ä¸è¯´æ˜
# =====================================
st.title("ğŸ“Š æ¨¡æ‹ŸProjectï¼šäººäº‹ç”¨åˆåŒè®°å½•è¡¨è‡ªåŠ¨å®¡æ ¸ç³»ç»Ÿï¼ˆâš¡å‘é‡åŒ–ç‰ˆ + å››Sheet + æ¼å¡«æ£€æŸ¥ + é©»åº—å®¢æˆ·ç‰ˆï¼‰")

# =====================================
# ğŸ“‚ ä¸Šä¼ æ–‡ä»¶åŒºï¼šè¦æ±‚ä¸Šä¼  5 ä¸ª xlsx æ–‡ä»¶
# =====================================
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼ ä»¥ä¸‹æ–‡ä»¶ï¼šè®°å½•è¡¨ã€æ”¾æ¬¾æ˜ç»†ã€å­—æ®µã€äºŒæ¬¡æ˜ç»†ã€é‡å¡æ•°æ®",
    type="xlsx",
    accept_multiple_files=True
)

if not uploaded_files or len(uploaded_files) < 5:
    st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ 5 ä¸ªæ–‡ä»¶åç»§ç»­")
    st.stop()
else:
    st.success("âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ")

# =====================================
# ğŸ§° å·¥å…·å‡½æ•°åŒºï¼ˆåˆ—åæ¨¡ç³ŠåŒ¹é…ã€æ•°æ®æ¸…æ´—ï¼‰
# =====================================

def normalize_colname(c): 
    """å»é™¤ç©ºæ ¼ã€ç»Ÿä¸€å°å†™"""
    return str(c).strip().lower()

def find_col(df, keyword, exact=False):
    """æŒ‰å…³é”®å­—åŒ¹é…åˆ—åï¼ˆæ”¯æŒæ¨¡ç³Šï¼‰"""
    key = keyword.strip().lower()
    for col in df.columns:
        cname = normalize_colname(col)
        if (exact and cname == key) or (not exact and key in cname):
            return col
    return None

def find_file(files_list, keyword):
    """æŒ‰æ–‡ä»¶åå…³é”®å­—æŸ¥æ‰¾ä¸Šä¼ æ–‡ä»¶"""
    for f in files_list:
        if keyword in f.name:
            return f
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„æ–‡ä»¶")

def find_sheet(xls, keyword):
    """æŒ‰sheetåå…³é”®å­—æŸ¥æ‰¾"""
    for s in xls.sheet_names:
        if keyword in s:
            return s
    raise ValueError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„sheet")

def normalize_num(val):
    """é€šç”¨æ•°å€¼è§£æå‡½æ•°"""
    if pd.isna(val): return np.nan
    s = str(val).replace(",", "").strip()
    if s in ["", "-", "nan"]: return np.nan
    try:
        if "%" in s:
            return float(s.replace("%", "")) / 100
        return float(s)
    except ValueError:
        return np.nan

def normalize_text(val):
    """æ–‡æœ¬é¢„å¤„ç†ï¼Œå»ç©ºç™½ã€å¤§å°å†™ç»Ÿä¸€"""
    if pd.isna(val): return ""
    return str(val).strip().lower().replace(".0", "")

def compare_fields_vectorized(main_df, ref_df, contract_col_main, contract_col_ref, mapping_dict, tolerance_dict=None):
    """
    âš¡ å‘é‡åŒ–å­—æ®µæ¯”å¯¹ï¼šä¸€æ¬¡æ€§ merge åˆåŒå·å¹¶æ‰¹é‡è®¡ç®—é”™è¯¯æ ‡è®°ã€‚
    è¿”å›ï¼š
        merged_df: åˆå¹¶åä¸»è¡¨æ•°æ®
        error_mask: æ¯ä¸ªå­—æ®µçš„å¸ƒå°”é”™è¯¯çŸ©é˜µ
    """
    tolerance_dict = tolerance_dict or {}
    df = main_df.copy()
    ref = ref_df.copy()

    # åˆåŒå·æ ‡å‡†åŒ–
    df['_åˆåŒå·_'] = df[contract_col_main].astype(str).str.strip()
    ref['_åˆåŒå·_'] = ref[contract_col_ref].astype(str).str.strip()

    # å·¦è¿æ¥å¯¹é½å‚è€ƒæ•°æ®
    merged = pd.merge(df, ref, on="_åˆåŒå·_", suffixes=("", "_ref"), how="left")

    # åˆå§‹åŒ–é”™è¯¯æ ‡è®°çŸ©é˜µ
    error_mask = pd.DataFrame(False, index=merged.index, columns=mapping_dict.keys())

    for main_kw, ref_kw in mapping_dict.items():
        main_col = find_col(df, main_kw)
        ref_col = find_col(ref, ref_kw)
        if not main_col or not ref_col:
            continue

        a = merged[main_col]
        b = merged[f"{ref_col}_ref"]

        # æ—¥æœŸå­—æ®µæ¯”è¾ƒ
        if "æ—¥æœŸ" in main_kw or "æ—¶é—´" in main_kw or "æ—¥æœŸ" in ref_kw or "æ—¶é—´" in ref_kw:
            a_dt = pd.to_datetime(a, errors='coerce')
            b_dt = pd.to_datetime(b, errors='coerce')
            mismatch = ~((a_dt.dt.date == b_dt.dt.date) | (a_dt.isna() & b_dt.isna()))

        # æ•°å€¼å­—æ®µæ¯”è¾ƒ
        elif a.apply(lambda x: str(x).replace('.', '', 1).isdigit()).any():
            a_num = pd.to_numeric(a.astype(str).str.replace(",", ""), errors="coerce")
            b_num = pd.to_numeric(b.astype(str).str.replace(",", ""), errors="coerce")
            tol = tolerance_dict.get(main_kw, 1e-6)
            mismatch = (a_num - b_num).abs() > tol
            mismatch |= (a_num.isna() ^ b_num.isna())

        # æ–‡æœ¬å­—æ®µæ¯”è¾ƒ
        else:
            a_norm = a.astype(str).str.strip().str.lower().replace(".0", "")
            b_norm = b.astype(str).str.strip().str.lower().replace(".0", "")
            mismatch = ~(a_norm == b_norm)

        error_mask[main_kw] = mismatch.fillna(False)

    merged["_é”™è¯¯æ•°_"] = error_mask.sum(axis=1)
    merged["_æ˜¯å¦é”™è¯¯_"] = merged["_é”™è¯¯æ•°_"] > 0

    return merged, error_mask

# =====================================
# ğŸ§® å•sheetæ£€æŸ¥å‡½æ•°ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰
# =====================================
def check_one_sheet(sheet_keyword):
    start_time = time.time()
    xls_main = pd.ExcelFile(main_file)

    # è¯»å–ç›®æ ‡sheet
    try:
        target_sheet = find_sheet(xls_main, sheet_keyword)
    except ValueError:
        st.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ…å«ã€Œ{sheet_keyword}ã€çš„sheetï¼Œè·³è¿‡ã€‚")
        return 0, 0, 0, set()

    main_df = pd.read_excel(xls_main, sheet_name=target_sheet, header=1)
    contract_col_main = find_col(main_df, "åˆåŒ")
    if not contract_col_main:
        st.error(f"âŒ åœ¨ã€Œ{sheet_keyword}ã€ä¸­æœªæ‰¾åˆ°åˆåŒåˆ—ã€‚")
        return 0, 0, 0, set()

    # === å‘é‡åŒ–æ‰§è¡Œå¯¹æ¯” ===
    merged_fk, mask_fk = compare_fields_vectorized(main_df, fk_df, contract_col_main, contract_col_fk, mapping_fk)
    merged_zd, mask_zd = compare_fields_vectorized(main_df, zd_df, contract_col_main, contract_col_zd, mapping_zd, tolerance_dict={"ä¿è¯é‡‘æ¯”ä¾‹":0.005})
    merged_ec, mask_ec = compare_fields_vectorized(main_df, ec_df, contract_col_main, contract_col_ec, mapping_ec)
    merged_zk, mask_zk = compare_fields_vectorized(main_df, zk_df, contract_col_main, contract_col_zk, mapping_zk)

    # ç»Ÿè®¡é”™è¯¯ä¸å‡ºç°è¿‡çš„åˆåŒå·
    combined_error_mask = mask_fk | mask_zd | mask_ec | mask_zk
    error_rows = combined_error_mask.any(axis=1)
    total_errors = error_rows.sum()
    contracts_seen = set(main_df[contract_col_main].dropna().astype(str).str.strip())

    elapsed = time.time() - start_time
    st.success(f"âœ… {sheet_keyword} æ£€æŸ¥å®Œæˆï¼Œå…± {total_errors} å¤„é”™è¯¯ï¼Œç”¨æ—¶ {elapsed:.2f} ç§’ã€‚")

    # === å¯¼å‡ºæ ‡æ³¨ç‰ˆï¼ˆä»…é”™è¯¯è¡Œé»„è‰²æ ‡è®°ï¼‰ ===
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
    wb = Workbook()
    ws = wb.active
    for c_idx, c in enumerate(main_df.columns, 1): ws.cell(1, c_idx, c)
    for r_idx, row in enumerate(main_df.itertuples(index=False), 2):
        for c_idx, v in enumerate(row, 1):
            ws.cell(r_idx, c_idx, v)
        if error_rows.iloc[r_idx-2]:
            for c_idx in range(1, len(main_df.columns)+1):
                ws.cell(r_idx, c_idx).fill = yellow_fill

    output = BytesIO()
    wb.save(output)
    output.seek(0)
    st.download_button(f"ğŸ“¥ ä¸‹è½½ {sheet_keyword} å®¡æ ¸æ ‡æ³¨ç‰ˆ", output, f"è®°å½•è¡¨_{sheet_keyword}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx")

    return total_errors, elapsed, 0, contracts_seen

# =====================================
# ğŸ“– æ–‡ä»¶è¯†åˆ«ä¸åŠ è½½
# =====================================
main_file = find_file(uploaded_files, "è®°å½•è¡¨")
fk_file = find_file(uploaded_files, "æ”¾æ¬¾æ˜ç»†")
zd_file = find_file(uploaded_files, "å­—æ®µ")
ec_file = find_file(uploaded_files, "äºŒæ¬¡æ˜ç»†")
zk_file = find_file(uploaded_files, "é‡å¡æ•°æ®")

fk_df = pd.read_excel(pd.ExcelFile(fk_file), sheet_name=find_sheet(pd.ExcelFile(fk_file), "æœ¬å¸"))
zd_df = pd.read_excel(pd.ExcelFile(zd_file), sheet_name=find_sheet(pd.ExcelFile(zd_file), "é‡å¡"))
ec_df = pd.read_excel(ec_file)
zk_df = pd.read_excel(zk_file)

contract_col_fk = find_col(fk_df, "åˆåŒ")
contract_col_zd = find_col(zd_df, "åˆåŒ")
contract_col_ec = find_col(ec_df, "åˆåŒ")
contract_col_zk = find_col(zk_df, "åˆåŒ")

# å¯¹ç…§å­—æ®µæ˜ å°„
mapping_fk = {"æˆä¿¡æ–¹": "æˆä¿¡", "ç§Ÿèµæœ¬é‡‘": "æœ¬é‡‘", "ç§ŸèµæœŸé™æœˆ": "ç§ŸèµæœŸé™æœˆ", "å®¢æˆ·ç»ç†": "å®¢æˆ·ç»ç†", "èµ·ç§Ÿæ”¶ç›Šç‡": "æ”¶ç›Šç‡", "ä¸»è½¦å°æ•°": "ä¸»è½¦å°æ•°", "æŒ‚è½¦å°æ•°": "æŒ‚è½¦å°æ•°"}
mapping_zd = {"ä¿è¯é‡‘æ¯”ä¾‹": "ä¿è¯é‡‘æ¯”ä¾‹_2", "é¡¹ç›®ææŠ¥äºº": "ææŠ¥", "èµ·ç§Ÿæ—¶é—´": "èµ·ç§Ÿæ—¥_å•†", "ç§ŸèµæœŸé™æœˆ": "æ€»æœŸæ•°_å•†_èµ„äº§", "æ‰€å±çœåŒº": "åŒºåŸŸ", "åŸå¸‚ç»ç†": "åŸå¸‚ç»ç†"}
mapping_ec = {"äºŒæ¬¡æ—¶é—´": "å‡ºæœ¬æµç¨‹æ—¶é—´"}
mapping_zk = {"æˆä¿¡æ–¹": "æˆä¿¡æ–¹"}

# =====================================
# ğŸ§¾ å¤šsheetå¾ªç¯ + é©»åº—å®¢æˆ·è¡¨
# =====================================
sheet_keywords = ["äºŒæ¬¡", "éƒ¨åˆ†æ‹…ä¿", "éšå·", "é©»åº—å®¢æˆ·"]
total_all = elapsed_all = 0
contracts_seen_all_sheets = set()

for kw in sheet_keywords:
    count, used, skipped, seen = check_one_sheet(kw)
    total_all += count
    elapsed_all += used
    contracts_seen_all_sheets.update(seen)

st.success(f"ğŸ¯ å…¨éƒ¨å®¡æ ¸å®Œæˆï¼Œå…± {total_all} å¤„é”™è¯¯ï¼Œæ€»è€—æ—¶ {elapsed_all:.2f} ç§’ã€‚")

# =====================================
# ğŸ•µï¸ æ¼å¡«æ£€æŸ¥éƒ¨åˆ†
# =====================================
field_contracts = zd_df[contract_col_zd].dropna().astype(str).str.strip()
col_car_manager = find_col(zd_df, "æ˜¯å¦è½¦ç®¡å®¶", exact=True)
col_bonus_type = find_col(zd_df, "ææˆç±»å‹", exact=True)

missing_mask = ~field_contracts.isin(contracts_seen_all_sheets)
if col_car_manager:
    missing_mask &= ~(zd_df[col_car_manager].astype(str).str.strip().str.lower() == "æ˜¯")
if col_bonus_type:
    missing_mask &= ~zd_df[col_bonus_type].astype(str).str.strip().isin(["è”åˆç§Ÿèµ", "é©»åº—"])

zd_df_missing = zd_df.copy()
zd_df_missing["æ¼å¡«æ£€æŸ¥"] = ""
zd_df_missing.loc[missing_mask, "æ¼å¡«æ£€æŸ¥"] = "â— æ¼å¡«"
æ¼å¡«åˆåŒæ•° = zd_df_missing["æ¼å¡«æ£€æŸ¥"].eq("â— æ¼å¡«").sum()
st.warning(f"âš ï¸ å…±å‘ç° {æ¼å¡«åˆåŒæ•°} ä¸ªåˆåŒåœ¨è®°å½•è¡¨ä¸­æœªå‡ºç°ï¼ˆå·²æ’é™¤è½¦ç®¡å®¶ã€è”åˆç§Ÿèµã€é©»åº—ï¼‰")

# =====================================
# ğŸ“¤ å¯¼å‡ºå«æ¼å¡«æ ‡æ³¨ä¸ä»…æ¼å¡«
# =====================================
yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

def write_xlsx(df):
    wb = Workbook()
    ws = wb.active
    for c_idx, c in enumerate(df.columns, 1):
        ws.cell(1, c_idx, c)
    for r_idx, row in enumerate(df.itertuples(index=False), 2):
        for c_idx, v in enumerate(row, 1):
            ws.cell(r_idx, c_idx, v)
            if df.columns[c_idx-1] == "æ¼å¡«æ£€æŸ¥" and v == "â— æ¼å¡«":
                ws.cell(r_idx, c_idx).fill = yellow_fill
    output = BytesIO()
    wb.save(output)
    output.seek(0)
    return output

st.download_button("ğŸ“¥ ä¸‹è½½å­—æ®µè¡¨æ¼å¡«æ ‡æ³¨ç‰ˆ", write_xlsx(zd_df_missing), "å­—æ®µè¡¨_æ¼å¡«æ ‡æ³¨ç‰ˆ.xlsx")
only_missing = zd_df_missing[zd_df_missing["æ¼å¡«æ£€æŸ¥"] == "â— æ¼å¡«"]
if not only_missing.empty:
    st.download_button("ğŸ“¥ ä¸‹è½½ä»…æ¼å¡«å­—æ®µè¡¨", write_xlsx(only_missing), "å­—æ®µè¡¨_ä»…æ¼å¡«.xlsx")

# =====================================
# âœ… å®Œæˆæç¤º
# =====================================
st.success("âœ… æ‰€æœ‰æ£€æŸ¥ã€æ ‡æ³¨ä¸å¯¼å‡ºå®Œæˆï¼ï¼ˆå·²å¯ç”¨å‘é‡åŒ–åŠ é€Ÿï¼Œå¤„ç†é€Ÿåº¦æå‡çº¦10~20å€ï¼‰")

