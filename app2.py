# =====================================
# Streamlit Web App: åˆåŒè®°å½•è¡¨è‡ªåŠ¨å®¡æ ¸ï¼ˆå‘é‡åŒ– + å››Sheet + æ¼å¡«æ£€æŸ¥ + é©»åº—å®¢æˆ·ç‰ˆï¼‰
# ä¿®æ­£ç‰ˆï¼šä¿®å¤åˆ—è¯»å–ã€_ref å›é€€ã€exact åŒ¹é…ã€ç´¢å¼•å¯¹é½
# =====================================

import streamlit as st
import pandas as pd
import time
from openpyxl import load_workbook, Workbook
from openpyxl.styles import PatternFill
from io import BytesIO

# =====================================
# ğŸ åº”ç”¨æ ‡é¢˜
# =====================================
st.title("ğŸ“Š æ¨¡æ‹Ÿå®é™…è¿ç”¨ç¯å¢ƒProjectï¼šäººäº‹ç”¨åˆåŒè®°å½•è¡¨è‡ªåŠ¨å®¡æ ¸ç³»ç»Ÿï¼ˆå‘é‡åŒ– + å››Sheet + æ¼å¡«æ£€æŸ¥ + é©»åº—å®¢æˆ·ç‰ˆï¼‰")

# =====================================
# ğŸ“‚ ä¸Šä¼ æ–‡ä»¶
# =====================================
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼ ä»¥ä¸‹æ–‡ä»¶ï¼šè®°å½•è¡¨ã€æ”¾æ¬¾æ˜ç»†ã€å­—æ®µã€äºŒæ¬¡æ˜ç»†ã€é‡å¡æ•°æ®",
    type="xlsx",
    accept_multiple_files=True
)
if not uploaded_files or len(uploaded_files) < 5:
    st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ 5 ä¸ªæ–‡ä»¶åç»§ç»­")
    st.stop()
else:
    st.success("âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ")

# =====================================
# ğŸ§° å·¥å…·å‡½æ•°
# =====================================
def find_file(files_list, keyword):
    for f in files_list:
        if keyword in f.name:
            return f
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„æ–‡ä»¶")

def normalize_colname(c):
    return str(c).strip().lower()

def find_col(df, keyword, exact=False):
    """åœ¨ DataFrame ä¸­æ ¹æ®å…³é”®å­—ï¼ˆæˆ– exact ç²¾ç¡®åŒ¹é…ï¼‰æŸ¥æ‰¾åˆ—åï¼ˆè¿”å›çœŸå®åˆ—åï¼‰"""
    if df is None or len(df.columns) == 0:
        return None
    key = keyword.strip().lower()
    for col in df.columns:
        cname = normalize_colname(col)
        if (exact and cname == key) or (not exact and key in cname):
            return col
    return None

def find_sheet(xls, keyword):
    for s in xls.sheet_names:
        if keyword in s:
            return s
    raise ValueError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„sheet")

def normalize_num(val):
    if pd.isna(val): return None
    s = str(val).replace(",", "").strip()
    if s in ["", "-", "nan"]: return None
    try:
        if "%" in s:
            return float(s.replace("%", "")) / 100
        return float(s)
    except Exception:
        return None

def same_date_ymd(a, b):
    try:
        da = pd.to_datetime(a, errors='coerce')
        db = pd.to_datetime(b, errors='coerce')
        if pd.isna(da) or pd.isna(db): return False
        return (da.year, da.month, da.day) == (db.year, db.month, db.day)
    except Exception:
        return False

def read_excel_clean(file, sheet_name=None, header=0):
    """
    æ›´ç¨³å¥çš„è¯»å–ï¼š
      - å¦‚æœç»™å®š sheet_name ä¸”å­˜åœ¨åˆ™è¯»å–å®ƒ
      - å¦åˆ™è¯»å–ç¬¬ä¸€ä¸ª sheet
      - header å¯æŒ‡å®šï¼ˆè®°å½•è¡¨ header=1ï¼‰
      - åˆ—åå»å‰åç©ºæ ¼
      - å‘ç”Ÿå¼‚å¸¸è¿”å›ç©º DataFrameï¼ˆå¹¶åœ¨ Streamlit ä¸­æ˜¾ç¤ºé”™è¯¯ï¼‰
    """
    try:
        xl = pd.ExcelFile(file)
        if sheet_name is None:
            sheet = xl.sheet_names[0]
        else:
            # å°è¯•æ¨¡ç³ŠåŒ¹é… sheet_nameï¼ˆåŒ…å«å…³ç³»ï¼‰
            sheet = None
            for s in xl.sheet_names:
                if sheet_name in s:
                    sheet = s
                    break
            if sheet is None:
                # å›é€€åˆ°ç¬¬ä¸€ä¸ª sheetï¼Œä½†æç¤º
                sheet = xl.sheet_names[0]
                st.info(f"âš ï¸ æŒ‡å®š sheet åã€Œ{sheet_name}ã€æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª sheetï¼š{sheet}")
        df = pd.read_excel(xl, sheet_name=sheet, header=header)
        # è§„èŒƒåŒ–åˆ—åï¼ˆå»å‰åç©ºæ ¼ï¼‰
        df.columns = [str(c).strip() for c in df.columns]
        return df
    except Exception as e:
        st.error(f"âŒ è¯»å– Excel å¤±è´¥ï¼š{e}")
        return pd.DataFrame()

# =====================================
# ğŸ“– æ–‡ä»¶å®šä½ä¸è¯»å–ï¼ˆè®°å½•è¡¨ header=1ï¼Œå‚è€ƒè¡¨ header=0ï¼‰
# =====================================
main_file = find_file(uploaded_files, "è®°å½•è¡¨")
fk_file = find_file(uploaded_files, "æ”¾æ¬¾æ˜ç»†")
zd_file = find_file(uploaded_files, "å­—æ®µ")
ec_file = find_file(uploaded_files, "äºŒæ¬¡æ˜ç»†")
zk_file = find_file(uploaded_files, "é‡å¡æ•°æ®")

xls_main = pd.ExcelFile(main_file)
sheet_keywords = ["äºŒæ¬¡", "éƒ¨åˆ†æ‹…ä¿", "éšå·", "é©»åº—å®¢æˆ·"]

# å‚è€ƒè¡¨ï¼šç¬¬ä¸€è¡Œä¸ºè¡¨å¤´
fk_df = read_excel_clean(fk_file, sheet_name=find_sheet(pd.ExcelFile(fk_file), "æœ¬å¸"), header=0)
zd_df = read_excel_clean(zd_file, sheet_name=find_sheet(pd.ExcelFile(zd_file), "é‡å¡"), header=0)
ec_df = read_excel_clean(ec_file, header=0)
zk_df = read_excel_clean(zk_file, header=0)

# =====================================
# ğŸ“Œ åˆåŒåˆ—ï¼ˆå‚è€ƒè¡¨ï¼šä½¿ç”¨ exact=True æ›´ç¨³å¥ï¼‰
# =====================================
contract_col_fk = find_col(fk_df, "åˆåŒ", exact=True)
contract_col_zd = find_col(zd_df, "åˆåŒ", exact=True)
contract_col_ec = find_col(ec_df, "åˆåŒ", exact=True)
contract_col_zk = find_col(zk_df, "åˆåŒ", exact=True)

# =====================================
# ğŸ”— å­—æ®µæ˜ å°„ï¼ˆä¸»è¡¨å -> å‚è€ƒè¡¨åï¼‰
# =====================================
mapping_fk = {"æˆä¿¡æ–¹":"æˆä¿¡", "ç§Ÿèµæœ¬é‡‘":"æœ¬é‡‘", "ç§ŸèµæœŸé™æœˆ":"ç§ŸèµæœŸé™æœˆ",
              "å®¢æˆ·ç»ç†":"å®¢æˆ·ç»ç†", "èµ·ç§Ÿæ”¶ç›Šç‡":"æ”¶ç›Šç‡", "ä¸»è½¦å°æ•°":"ä¸»è½¦å°æ•°", "æŒ‚è½¦å°æ•°":"æŒ‚è½¦å°æ•°"}

mapping_zd = {"ä¿è¯é‡‘æ¯”ä¾‹":"ä¿è¯é‡‘æ¯”ä¾‹_2", "é¡¹ç›®ææŠ¥äºº":"ææŠ¥", "èµ·ç§Ÿæ—¶é—´":"èµ·ç§Ÿæ—¥_å•†",
              "ç§ŸèµæœŸé™æœˆ":"æ€»æœŸæ•°_å•†_èµ„äº§", "èµ·ç§Ÿæ”¶ç›Šç‡":"XIRR_å•†_èµ·ç§Ÿ", "æ‰€å±çœåŒº":"åŒºåŸŸ", "åŸå¸‚ç»ç†":"åŸå¸‚ç»ç†"}

mapping_ec = {"äºŒæ¬¡æ—¶é—´":"å‡ºæœ¬æµç¨‹æ—¶é—´"}
mapping_zk = {"æˆä¿¡æ–¹":"æˆä¿¡æ–¹"}

# =====================================
# âš¡ å‘é‡åŒ–æ¯”å¯¹å‡½æ•°ï¼ˆæ›´ç¨³å¥ï¼‰
# =====================================
def compare_fields_vectorized(main_df, ref_df, main_contract_col, ref_contract_col, mapping_dict, tolerance_dict=None):
    """
    å‘é‡åŒ–æ¯”å¯¹ï¼ˆç¨³å¥ç‰ˆï¼‰ï¼š
    - å…ˆé€šè¿‡ find_col æ‰¾åˆ°ä¸»/å‚è€ƒè¡¨çœŸå®åˆ—åï¼ˆæ”¯æŒ exact æŒ‡å®šï¼‰
    - merge åå®‰å…¨è¯»å–å‚è€ƒåˆ—ï¼ˆå°è¯• ref_col_ref -> å›é€€åˆ° ref_colï¼‰
    - è¿”å› mergedï¼ˆç”¨äº debug / åç»­æ‰©å±•ï¼‰ä¸ maskï¼ˆåˆ—åä¸ºä¸»è¡¨çœŸå®åˆ—åï¼‰
    """
    tolerance_dict = tolerance_dict or {}

    # ä¿è¯ main_df çš„ç´¢å¼•è¿ç»­ 0..n-1ï¼Œä¾¿äº later ä½¿ç”¨ä½ç½®ç´¢å¼•
    main_df = main_df.reset_index(drop=True)
    main_df_clean = main_df.copy()
    main_df_clean[main_contract_col] = main_df_clean[main_contract_col].astype(str).str.strip()

    ref_df_clean = ref_df.copy()
    ref_df_clean[ref_contract_col] = ref_df_clean[ref_contract_col].astype(str).str.strip()

    # å‡†å¤‡ ref å­è¡¨ï¼ˆè‹¥ç¼ºåˆ—åˆ™æç¤ºå¹¶è¿”å›ç©º maskï¼‰
    ref_cols_needed = [ref_contract_col] + list(mapping_dict.values())
    missing_cols = [c for c in ref_cols_needed if c not in ref_df_clean.columns]
    if missing_cols:
        st.error(f"âŒ å‚è€ƒè¡¨ç¼ºå°‘åˆ—: {missing_cols}")
        mask_empty = pd.DataFrame(False, index=main_df.index, columns=[
            # æŠŠ mask åˆ—åè®¾ä¸ºä¸»è¡¨çœŸå®åˆ—åï¼ˆå°½é‡æ‰¾å‡ºï¼‰
            (find_col(main_df, mk, exact=(mk in ["å®¢æˆ·ç»ç†","åŸå¸‚ç»ç†"])) or mk)
            for mk in mapping_dict.keys()
        ])
        return main_df_clean.copy(), mask_empty

    ref_sub = ref_df_clean[ref_cols_needed]

    # mergeï¼ˆå·¦è¿æ¥ï¼‰
    merged = main_df_clean.merge(ref_sub, how="left",
                                 left_on=main_contract_col, right_on=ref_contract_col,
                                 suffixes=("", "_ref"))

    # mask åˆ—åä½¿ç”¨ä¸»è¡¨çœŸå®åˆ—åï¼ˆé¿å… mapping é”®ä¸å®é™…åˆ—åä¸ä¸€è‡´é€ æˆæ··ä¹±ï¼‰
    actual_main_cols = []
    for mk in mapping_dict.keys():
        actual = find_col(main_df, mk, exact=(mk in ["å®¢æˆ·ç»ç†","åŸå¸‚ç»ç†"]))
        if actual is None:
            # å›é€€ä½¿ç”¨ mapping key æœ¬èº«ï¼ˆå¯èƒ½ä¸»è¡¨ç¼ºåˆ—ï¼‰
            actual = mk
        actual_main_cols.append(actual)

    mask = pd.DataFrame(False, index=merged.index, columns=actual_main_cols)

    # é€å­—æ®µåšå‘é‡åŒ–æ¯”è¾ƒï¼ˆä¸ä½¿ç”¨ Python å±‚é¢çš„é€è¡Œå¾ªç¯ï¼‰
    for mk, rv in mapping_dict.items():
        main_col = find_col(main_df, mk, exact=(mk in ["å®¢æˆ·ç»ç†","åŸå¸‚ç»ç†"]))
        ref_col = find_col(ref_df, rv, exact=(rv in ["å®¢æˆ·ç»ç†","åŸå¸‚ç»ç†"]))
        if main_col is None:
            # ä¸»è¡¨æ²¡æœ‰æ­¤åˆ—ï¼Œè·³è¿‡ï¼ˆå·²åœ¨ mask ä¸­ä¿ç•™ä¸º Falseï¼‰
            continue
        # ç¡®å®šå‚è€ƒåˆ—åœ¨ merged ä¸­çš„åˆ—åï¼šå…ˆå°è¯•å¸¦ _ref çš„ç‰ˆæœ¬ï¼ˆå½“åˆ—åå†²çªæ—¶ pandas ä¼šæ·»åŠ åç¼€ï¼‰
        ref_col_in_merged = f"{ref_col}_ref" if f"{ref_col}_ref" in merged.columns else ref_col

        # è‹¥å‚è€ƒåˆ—ä¸åœ¨ mergedï¼ˆæå°‘è§ï¼‰ï¼Œå°†æ•´åˆ—æ ‡ä¸º False å¹¶ç»§ç»­
        if ref_col_in_merged not in merged.columns:
            continue

        main_vals = merged[main_col]
        ref_vals = merged[ref_col_in_merged]

        is_date_col = any(k in mk for k in ["æ—¥æœŸ", "æ—¶é—´"]) or any(k in rv for k in ["æ—¥æœŸ", "æ—¶é—´"])
        tol = tolerance_dict.get(mk, 0)
        exact_match = mk in ["å®¢æˆ·ç»ç†", "åŸå¸‚ç»ç†"]

        # æ—¥æœŸæ¯”è¾ƒï¼ˆå‘é‡åŒ–ï¼‰
        if is_date_col:
            main_dt = pd.to_datetime(main_vals, errors='coerce').dt.normalize()
            ref_dt = pd.to_datetime(ref_vals, errors='coerce').dt.normalize()
            # æ³¨æ„ï¼šNaT æ¯”è¾ƒä¼šè¿”å› Falseï¼Œæ‰€ä»¥æˆ‘ä»¬æŠŠ NaT-NaT è§†ä¸ºç›¸ç­‰
            date_mismatch = ~(main_dt.eq(ref_dt) | (main_dt.isna() & ref_dt.isna()))
            mask.loc[date_mismatch.index, main_col] = date_mismatch.fillna(False)
            continue

        # éæ—¥æœŸï¼šå…ˆå°è¯•æ•°å€¼æ¯”è¾ƒï¼ˆä½¿ç”¨ normalize_num å‘é‡åŒ–ï¼‰
        main_num = main_vals.apply(normalize_num)
        ref_num = ref_vals.apply(normalize_num)

        # æ•°å€¼éƒ½å­˜åœ¨æ—¶æŒ‰å®¹å·®æ¯”è¾ƒ
        both_num = main_num.notna() & ref_num.notna()
        num_mismatch = pd.Series(False, index=merged.index)
        if both_num.any():
            # æ³¨æ„ï¼š main_num/ref_num æ˜¯æ··åˆç±»å‹çš„ Seriesï¼ˆå¯èƒ½åŒ…å« Noneï¼‰ï¼Œå…ˆè½¬æ¢ä¸º float where possible
            try:
                # compute difference for numeric positions
                diff = (main_num - ref_num).abs()
                num_mismatch = both_num & (diff > tol)
            except Exception:
                # è‹¥å‡æ³•å¤±è´¥ï¼ˆéæ•°å€¼ï¼‰ï¼Œä¿æŒ False
                num_mismatch = both_num & False

        # æ–‡æœ¬/å…¶ä»–æ¯”è¾ƒï¼ˆå« exact é€»è¾‘ï¼‰
        # å½“æ•°å€¼æ¯”è¾ƒä¸é€‚ç”¨ï¼ˆä»»ä¸€ä¸º Naï¼‰ï¼ŒæŒ‰å­—ç¬¦ä¸²æ¯”è¾ƒï¼ˆå¿½ç•¥å¤§å°å†™ä¸å°¾éƒ¨ .0ï¼‰
        text_mask = pd.Series(False, index=merged.index)
        non_num_positions = ~(both_num)
        if non_num_positions.any():
            a_str = main_vals.astype(str).fillna("").str.strip().str.lower().str.replace(".0", "")
            b_str = ref_vals.astype(str).fillna("").str.strip().str.lower().str.replace(".0", "")
            if exact_match:
                text_mask = non_num_positions & (~a_str.eq(b_str))
            else:
                text_mask = non_num_positions & (~a_str.eq(b_str))

        # Na/NotNa ä¸ç­‰ä¹Ÿè§†ä¸º mismatch
        nan_mismatch = (main_num.isna() ^ ref_num.isna())

        mask_col_result = num_mismatch | text_mask | nan_mismatch
        mask.loc[mask_col_result.index, main_col] = mask_col_result.fillna(False)

    return merged, mask

# =====================================
# ğŸ§® å• sheet æ£€æŸ¥å‡½æ•°ï¼ˆä½¿ç”¨å‘é‡åŒ–æ¯”å¯¹ï¼‰
# =====================================
def check_one_sheet(sheet_keyword):
    start_time = time.time()
    # è®°å½•è¡¨ sheet header=1ï¼ˆç¬¬äºŒè¡Œä¸ºåˆ—åï¼‰
    main_df = read_excel_clean(main_file, sheet_name=find_sheet(xls_main, sheet_keyword), header=1)
    # reset index 0..n-1 ä»¥ä¾¿ mask/merged å¯¹é½
    main_df = main_df.reset_index(drop=True)

    # å‡†å¤‡è¾“å‡º Excelï¼ˆä¿ç•™ç©ºè¡Œä»¥ä¸åŸç‰ˆä¸€è‡´ï¼šæ•°æ®ä» excel è¡Œ 3 å¼€å§‹ï¼‰
    output_path = f"è®°å½•è¡¨_{sheet_keyword}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx"
    empty_row = pd.DataFrame([[""] * len(main_df.columns)], columns=main_df.columns)
    pd.concat([empty_row, main_df], ignore_index=True).to_excel(output_path, index=False)
    wb = load_workbook(output_path)
    ws = wb.active

    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

    # åˆåŒåˆ—ï¼ˆå¼ºåˆ¶ exact åŒ¹é…ï¼‰
    global contract_col_main
    contract_col_main = find_col(main_df, "åˆåŒ", exact=False)
    if not contract_col_main:
        st.error(f"âŒ åœ¨ã€Œ{sheet_keyword}ã€ä¸­æœªæ‰¾åˆ°åˆåŒåˆ—ã€‚")
        return 0, None, 0, set()

    contracts_seen = set()
    progress = st.progress(0)
    status = st.empty()

    # å‘é‡åŒ–æ¯”å¯¹å››å¼ å‚è€ƒè¡¨
    merged_fk, mask_fk = compare_fields_vectorized(main_df, fk_df, contract_col_main, contract_col_fk, mapping_fk, tolerance_dict={})
    merged_zd, mask_zd = compare_fields_vectorized(main_df, zd_df, contract_col_main, contract_col_zd, mapping_zd, tolerance_dict={"ä¿è¯é‡‘æ¯”ä¾‹": 0.005})
    merged_ec, mask_ec = compare_fields_vectorized(main_df, ec_df, contract_col_main, contract_col_ec, mapping_ec, tolerance_dict={})
    merged_zk, mask_zk = compare_fields_vectorized(main_df, zk_df, contract_col_main, contract_col_zk, mapping_zk, tolerance_dict={})

    # åˆå¹¶æ‰€æœ‰ maskï¼ˆåˆ—åæ˜¯ä¸»è¡¨å®é™…åˆ—åï¼‰
    mask_all = pd.concat([mask_fk, mask_zd, mask_ec, mask_zk], axis=1).fillna(False)
    mask_any = mask_all.any(axis=1)

    # æ ‡çº¢ / æ ‡é»„ï¼ˆä½¿ç”¨ main_df çš„ä½ç½®ç´¢å¼• r_idx å¯¹åº” excel è¡Œ r_idx+3ï¼‰
    for r_idx, row in main_df.iterrows():
        contracts_seen.add(str(row[contract_col_main]).strip() if not pd.isna(row[contract_col_main]) else "")
        # æ ‡çº¢ï¼šé€åˆ—æ£€æŸ¥ mask_allï¼ˆmask_all åˆ—åæ˜¯ä¸»è¡¨å®é™…åˆ—åï¼‰
        for col in mask_all.columns:
            try:
                if mask_all.at[r_idx, col]:
                    c_idx = list(main_df.columns).index(col) + 1
                    ws.cell(r_idx + 3, c_idx).fill = red_fill
            except Exception:
                # è‹¥æŸäº› mask åˆ—ä¸åœ¨ä¸»è¡¨åˆ—ä¸­ï¼ˆæå°‘ï¼‰ï¼Œè·³è¿‡
                continue
        # æ ‡é»„åˆåŒå·ï¼ˆè‹¥è¯¥è¡Œä»»ä¸€åˆ—å‡ºé”™ï¼‰
        try:
            if mask_any.at[r_idx]:
                c_contract = list(main_df.columns).index(contract_col_main) + 1
                ws.cell(r_idx + 3, c_contract).fill = yellow_fill
        except Exception:
            pass

        # è¿›åº¦æ˜¾ç¤ºï¼ˆæ¯ 10 è¡Œåˆ·æ–°ï¼‰
        if (r_idx + 1) % 10 == 0:
            status.text(f"æ£€æŸ¥ã€Œ{sheet_keyword}ã€... {r_idx+1}/{len(main_df)}")
        progress.progress((r_idx + 1) / max(1, len(main_df)))

    # å¯¼å‡ºä¸‹è½½
    output = BytesIO()
    wb.save(output)
    output.seek(0)
    st.download_button(
        label=f"ğŸ“¥ ä¸‹è½½ {sheet_keyword} å®¡æ ¸æ ‡æ³¨ç‰ˆ",
        data=output,
        file_name=f"è®°å½•è¡¨_{sheet_keyword}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx"
    )

    total_errors = int(mask_any.sum())
    elapsed = time.time() - start_time
    st.success(f"âœ… {sheet_keyword} æ£€æŸ¥å®Œæˆï¼Œå…± {total_errors} å¤„é”™è¯¯ï¼Œç”¨æ—¶ {elapsed:.2f} ç§’ã€‚")
    return total_errors, elapsed, 0, contracts_seen

# =====================================
# ğŸ§¾ å¤š sheet å¾ªç¯
# =====================================
total_all = elapsed_all = skip_total = 0
contracts_seen_all_sheets = set()
for kw in sheet_keywords:
    count, used, skipped, seen = check_one_sheet(kw)
    total_all += count
    elapsed_all += used or 0
    skip_total += skipped
    contracts_seen_all_sheets.update(seen)

st.success(f"ğŸ¯ å…¨éƒ¨å®¡æ ¸å®Œæˆï¼Œå…± {total_all} å¤„é”™è¯¯ï¼Œæ€»è€—æ—¶ {elapsed_all:.2f} ç§’ã€‚")

# =====================================
# ğŸ•µï¸ æ¼å¡«æ£€æŸ¥ï¼ˆä¿ç•™åŸé€»è¾‘ï¼šæ’é™¤â€œæ˜¯å¦è½¦ç®¡å®¶=æ˜¯â€ä¸â€œææˆç±»å‹=è”åˆç§Ÿèµ/é©»åº—â€ï¼‰
# =====================================
field_contracts = zd_df[contract_col_zd].dropna().astype(str).str.strip()
col_car_manager = find_col(zd_df, "æ˜¯å¦è½¦ç®¡å®¶", exact=True)
col_bonus_type = find_col(zd_df, "ææˆç±»å‹", exact=True)

missing_contracts_mask = (~field_contracts.isin(contracts_seen_all_sheets))
if col_car_manager:
    missing_contracts_mask &= ~(zd_df[col_car_manager].astype(str).str.strip().str.lower() == "æ˜¯")
if col_bonus_type:
    missing_contracts_mask &= ~(
        zd_df[col_bonus_type].astype(str).str.strip().isin(["è”åˆç§Ÿèµ","é©»åº—"])
    )

zd_df_missing = zd_df.copy()
zd_df_missing["æ¼å¡«æ£€æŸ¥"] = ""
zd_df_missing.loc[missing_contracts_mask, "æ¼å¡«æ£€æŸ¥"] = "â— æ¼å¡«"
æ¼å¡«åˆåŒæ•° = zd_df_missing["æ¼å¡«æ£€æŸ¥"].eq("â— æ¼å¡«").sum()
st.warning(f"âš ï¸ å…±å‘ç° {æ¼å¡«åˆåŒæ•°} ä¸ªåˆåŒåœ¨è®°å½•è¡¨ä¸­æœªå‡ºç°ï¼ˆå·²æ’é™¤è½¦ç®¡å®¶ã€è”åˆç§Ÿèµã€é©»åº—ï¼‰")

# =====================================
# ğŸ“¤ å¯¼å‡ºå­—æ®µè¡¨ï¼ˆå«æ¼å¡«æ ‡æ³¨ + ä»…æ¼å¡«ï¼‰
# =====================================
yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
wb_all = Workbook()
ws_all = wb_all.active
for c_idx, c in enumerate(zd_df_missing.columns, 1):
    ws_all.cell(1, c_idx, c)
for r_idx, row in enumerate(zd_df_missing.itertuples(index=False), 2):
    for c_idx, v in enumerate(row, 1):
        ws_all.cell(r_idx, c_idx, v)
        if zd_df_missing.columns[c_idx-1] == "æ¼å¡«æ£€æŸ¥" and v == "â— æ¼å¡«":
            ws_all.cell(r_idx, c_idx).fill = yellow_fill
output_all = BytesIO()
wb_all.save(output_all)
output_all.seek(0)
st.download_button("ğŸ“¥ ä¸‹è½½å­—æ®µè¡¨æ¼å¡«æ ‡æ³¨ç‰ˆ", output_all, "å­—æ®µè¡¨_æ¼å¡«æ ‡æ³¨ç‰ˆ.xlsx")

zd_df_only_missing = zd_df_missing[zd_df_missing["æ¼å¡«æ£€æŸ¥"] == "â— æ¼å¡«"].copy()
if not zd_df_only_missing.empty:
    wb2 = Workbook()
    ws2 = wb2.active
    for c_idx, c in enumerate(zd_df_only_missing.columns, 1): ws2.cell(1, c_idx, c)
    for r_idx, row in enumerate(zd_df_only_missing.itertuples(index=False), 2):
        for c_idx, v in enumerate(row, 1):
            ws2.cell(r_idx, c_idx, v)
            if zd_df_only_missing.columns[c_idx-1] == "æ¼å¡«æ£€æŸ¥" and v == "â— æ¼å¡«":
                ws2.cell(r_idx, c_idx).fill = yellow_fill
    out2 = BytesIO()
    wb2.save(out2)
    out2.seek(0)
    st.download_button("ğŸ“¥ ä¸‹è½½ä»…æ¼å¡«å­—æ®µè¡¨", out2, "å­—æ®µè¡¨_ä»…æ¼å¡«.xlsx")

st.success("âœ… æ‰€æœ‰æ£€æŸ¥ã€æ ‡æ³¨ä¸å¯¼å‡ºå®Œæˆï¼")
