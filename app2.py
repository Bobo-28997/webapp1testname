# =====================================
# Streamlit Web App: æ¨¡æ‹ŸProjectï¼šäººäº‹ç”¨åˆåŒè®°å½•è¡¨è‡ªåŠ¨å®¡æ ¸ï¼ˆå››è¾“å‡ºè¡¨ç‰ˆ + æ¼å¡«æ£€æŸ¥ + é©»åº—å®¢æˆ·ç‰ˆï¼‰
# (V3: ç¼“å­˜ä¼˜åŒ–ç‰ˆ)
# =====================================

import streamlit as st
import pandas as pd
import time
from openpyxl import load_workbook, Workbook
from openpyxl.styles import PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows
from io import BytesIO

# =====================================
# ğŸ§° å·¥å…·å‡½æ•°åŒº (ä¸å˜)
# =====================================

def normalize_contract_key(series: pd.Series) -> pd.Series:
    """
    å¯¹åˆåŒå· Series è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œç”¨äºå®‰å…¨çš„ pd.merge æ“ä½œã€‚
    """
    s = series.astype(str)
    s = s.str.replace(r"\.0$", "", regex=True) 
    s = s.str.strip()
    s = s.str.upper() 
    s = s.str.replace('ï¼', '-', regex=False)
    s = s.str.replace(r'\s+', '', regex=True)
    return s

def find_file(files_list, keyword):
    """
    æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬æ¥æ”¶ Streamlit çš„ UploadedFile å¯¹è±¡åˆ—è¡¨
    """
    for f in files_list:
        if keyword in f.name:
            return f
    raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„æ–‡ä»¶")

def normalize_colname(c): return str(c).strip().lower()

def find_col(df, keyword, exact=False):
    key = keyword.strip().lower()
    for col in df.columns:
        cname = normalize_colname(col)
        if (exact and cname == key) or (not exact and key in cname):
            return col
    return None

def find_sheet(xls, keyword):
    for s in xls.sheet_names:
        if keyword in s:
            return s
    raise ValueError(f"âŒ æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯ã€Œ{keyword}ã€çš„sheet")

def normalize_num(val):
    if pd.isna(val): return None
    s = str(val).replace(",", "").strip()
    if s in ["", "-", "nan"]: return None
    try:
        if "%" in s: return float(s.replace("%", "")) / 100
        return float(s)
    except ValueError:
        return s

def prepare_ref_df(ref_df, mapping, prefix):
    # 1. æ‰¾åˆ°åˆåŒåˆ—
    contract_col = find_col(ref_df, "åˆåŒ") 
    if not contract_col:
        st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°'åˆåŒ'åˆ—ï¼Œè·³è¿‡æ­¤æ•°æ®æºã€‚")
        return pd.DataFrame(columns=['__KEY__'])
        
    std_df = pd.DataFrame()
    # 2. å½’ä¸€åŒ– Key
    std_df['__KEY__'] = normalize_contract_key(ref_df[contract_col])
    
    # 3. æå–å¹¶é‡å‘½å
    for main_kw, ref_kw in mapping.items():
        exact = (main_kw == "åŸå¸‚ç»ç†")
        ref_col_name = find_col(ref_df, ref_kw, exact=exact)
        
        if ref_col_name:
            s_ref_raw = ref_df[ref_col_name]
            # 4. (æ ¸å¿ƒ) å¹´è½¬æœˆé€»è¾‘
            if prefix == 'fk' and main_kw == 'ç§ŸèµæœŸé™':
                s_ref_transformed = pd.to_numeric(s_ref_raw, errors='coerce') * 12
                std_df[f'ref_{prefix}_{main_kw}'] = s_ref_transformed
            else:
                std_df[f'ref_{prefix}_{main_kw}'] = s_ref_raw
        else:
            st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°åˆ— (main: '{main_kw}', ref: '{ref_kw}')")

    std_df = std_df.drop_duplicates(subset=['__KEY__'], keep='first')
    return std_df

def compare_series_vec(s_main, s_ref, main_kw):
    merge_failed_mask = s_ref.isna() 
    main_is_na = pd.isna(s_main) | (s_main.astype(str).str.strip().isin(["", "nan", "None"]))
    ref_is_na = pd.isna(s_ref) | (s_ref.astype(str).str.strip().isin(["", "nan", "None"]))
    both_are_na = main_is_na & ref_is_na
    
    if any(k in main_kw for k in ["æ—¥æœŸ", "æ—¶é—´"]):
        d_main = pd.to_datetime(s_main, errors='coerce')
        d_ref = pd.to_datetime(s_ref, errors='coerce')
        valid_dates_mask = d_main.notna() & d_ref.notna()
        date_diff_mask = (d_main.dt.date != d_ref.dt.date)
        errors = valid_dates_mask & date_diff_mask
    else:
        s_main_norm = s_main.apply(normalize_num)
        s_ref_norm = s_ref.apply(normalize_num)
        main_is_na_norm = pd.isna(s_main_norm) | (s_main_norm.astype(str).str.strip().isin(["", "nan", "None"]))
        ref_is_na_norm = pd.isna(s_ref_norm) | (s_ref_norm.astype(str).str.strip().isin(["", "nan", "None"]))
        both_are_na_norm = main_is_na_norm & ref_is_na_norm
        is_num_main = s_main_norm.apply(lambda x: isinstance(x, (int, float)))
        is_num_ref = s_ref_norm.apply(lambda x: isinstance(x, (int, float)))
        both_are_num = is_num_main & is_num_ref
        errors = pd.Series(False, index=s_main.index)
        
        if both_are_num.any():
            num_main = s_main_norm[both_are_num].fillna(0)
            num_ref = s_ref_norm[both_are_num].fillna(0)
            diff = (num_main - num_ref).abs()
            
            # (æ ¸å¿ƒ) å®¹é”™é€»è¾‘
            if main_kw == "ä¿è¯é‡‘æ¯”ä¾‹":
                num_errors = (diff > 0.00500001)
            elif "ç§ŸèµæœŸé™" in main_kw:
                num_errors = (diff >= 1.0) 
            else:
                num_errors = (diff > 1e-6)
            
            errors.loc[both_are_num] = num_errors

        not_num_mask = ~both_are_num
        if not_num_mask.any():
            str_main = s_main_norm[not_num_mask].astype(str).str.strip().str.replace(r"\.0$", "", regex=True)
            str_ref = s_ref_norm[not_num_mask].astype(str).str.strip().str.replace(r"\.0$", "", regex=True)
            str_errors = (str_main != str_ref)
            errors.loc[not_num_mask] = str_errors
            
        errors = errors & ~both_are_na_norm

    final_errors = errors & ~both_are_na
    lookup_failure_mask = merge_failed_mask & ~main_is_na
    final_errors = final_errors & ~lookup_failure_mask
    return final_errors

# =====================================
# ğŸ§® (ä¿®æ”¹) å•sheetæ£€æŸ¥å‡½æ•° - ç°åœ¨è¿”å›æ–‡ä»¶
# =====================================
def check_one_sheet(sheet_keyword, main_file, ref_dfs_std_dict, mappings_all):
    start_time = time.time()
    xls_main = pd.ExcelFile(main_file)

    try:
        target_sheet = find_sheet(xls_main, sheet_keyword)
    except ValueError:
        st.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ…å«ã€Œ{sheet_keyword}ã€çš„sheetï¼Œè·³è¿‡ã€‚")
        return (0, None, 0, set()), {} # è¿”å› (stats, files_dict)

    try:
        main_df = pd.read_excel(xls_main, sheet_name=target_sheet, header=1)
    except Exception as e:
        st.error(f"âŒ è¯»å–ã€Œ{sheet_keyword}ã€æ—¶å‡ºé”™: {e}")
        return (0, None, 0, set()), {}
        
    if main_df.empty:
        st.warning(f"âš ï¸ ã€Œ{sheet_keyword}ã€ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
        return (0, None, 0, set()), {}

    contract_col_main = find_col(main_df, "åˆåŒ")
    if not contract_col_main:
        st.error(f"âŒ åœ¨ã€Œ{sheet_keyword}ã€ä¸­æœªæ‰¾åˆ°åˆåŒåˆ—ã€‚")
        return (0, None, 0, set()), {}

    # (è¿™éƒ¨åˆ†ä¸å˜ï¼šåœ¨å†…å­˜ä¸­åˆ›å»ºæ ‡æ³¨)
    output_path = f"æœˆé‡å¡_{sheet_keyword}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx"
    empty_row = pd.DataFrame([[""] * len(main_df.columns)], columns=main_df.columns)
    
    # --- å†™å…¥ä¸´æ—¶ BytesIO è€Œä¸æ˜¯ç£ç›˜ ---
    temp_output_stream = BytesIO()
    with pd.ExcelWriter(temp_output_stream, engine='openpyxl') as writer:
        pd.concat([empty_row, main_df], ignore_index=True).to_excel(writer, index=False, sheet_name=target_sheet)
    temp_output_stream.seek(0)
    
    wb = load_workbook(temp_output_stream)
    ws = wb[target_sheet] # ç¡®ä¿æ¿€æ´»æ­£ç¡®çš„ sheet
    # --- ç»“æŸä¿®æ”¹ ---
    
    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

    main_df['__ROW_IDX__'] = main_df.index
    main_df['__KEY__'] = normalize_contract_key(main_df[contract_col_main])
    contracts_seen = set(main_df['__KEY__'].dropna())

    merged_df = main_df.copy()
    for prefix, std_df in ref_dfs_std_dict.items():
        if not std_df.empty:
            merged_df = pd.merge(merged_df, std_df, on='__KEY__', how='left')
    
    total_errors = 0
    skip_city_manager = [0]
    errors_locations = set()
    row_has_error = pd.Series(False, index=merged_df.index) 

    # --- (ç§»é™¤ st.progress å’Œ st.status) ---
    # ...

    total_comparisons = sum(len(m[0]) for m in mappings_all.values())
    current_comparison = 0

    for prefix, (mapping, std_df) in mappings_all.items():
        if std_df.empty:
            current_comparison += len(mapping)
            continue
            
        for main_kw, ref_kw in mapping.items():
            current_comparison += 1
            # (ç§»é™¤ st.status)
            
            exact = (main_kw == "åŸå¸‚ç»ç†")
            main_col = find_col(main_df, main_kw, exact=exact)
            ref_col = f'ref_{prefix}_{main_kw}'

            if not main_col or ref_col not in merged_df.columns:
                continue 

            s_main = merged_df[main_col]
            s_ref = merged_df[ref_col]

            skip_mask = pd.Series(False, index=merged_df.index)
            if main_kw == "åŸå¸‚ç»ç†":
                na_strings = ["", "-", "nan", "none", "null"]
                skip_mask = pd.isna(s_ref) | s_ref.astype(str).str.strip().isin(na_strings)
                skip_city_manager[0] += skip_mask.sum()
            
            errors_mask = compare_series_vec(s_main, s_ref, main_kw)
            final_errors_mask = errors_mask & ~skip_mask
            
            if final_errors_mask.any():
                total_errors += final_errors_mask.sum() # <-- æˆ‘ä»¬åœ¨è¿™é‡Œé‡æ–°è®¡ç®—æ€»é”™è¯¯æ•°
                row_has_error |= final_errors_mask
                bad_indices = merged_df[final_errors_mask]['__ROW_IDX__']
                for idx in bad_indices:
                    errors_locations.add((idx, main_col))
            
            # (ç§»é™¤ st.progress)

    # (ç§»é™¤ st.status)
    
    # --- (9. æ ‡æ³¨é€»è¾‘ä¸å˜) ---
    original_cols_list = list(main_df.drop(columns=['__ROW_IDX__', '__KEY__']).columns)
    col_name_to_idx = {name: i + 1 for i, name in enumerate(original_cols_list)}

    for (row_idx, col_name) in errors_locations:
        if col_name in col_name_to_idx:
            ws.cell(row_idx + 3, col_name_to_idx[col_name]).fill = red_fill

    if contract_col_main in col_name_to_idx:
        contract_col_excel_idx = col_name_to_idx[contract_col_main]
        error_row_indices = merged_df[row_has_error]['__ROW_IDX__']
        for row_idx in error_row_indices:
            ws.cell(row_idx + 3, contract_col_excel_idx).fill = yellow_fill

    # --- (10. ä¿®æ”¹ä¸º return æ–‡ä»¶) ---
    output = BytesIO()
    wb.save(output)
    output.seek(0)
    
    # (ç§»é™¤ st.download_button)

    files_to_save = {
        "full_report": (f"æœˆé‡å¡_{sheet_keyword}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx", output),
        "error_report": (None, None)
    }
    output_errors_only = None

    # --- (11. ä¿®æ”¹ä¸º return æ–‡ä»¶) ---
    if row_has_error.any():
        try:
            df_errors_only = merged_df.loc[row_has_error, original_cols_list].copy()
            original_indices_with_error = merged_df.loc[row_has_error, '__ROW_IDX__']
            original_idx_to_new_excel_row = {
                original_idx: new_row_num 
                for new_row_num, original_idx in enumerate(original_indices_with_error, start=2)
            }
            wb_errors = Workbook()
            ws_errors = wb_errors.active
            for r in dataframe_to_rows(df_errors_only, index=False, header=True):
                ws_errors.append(r)
            for (original_row_idx, col_name) in errors_locations:
                if original_row_idx in original_idx_to_new_excel_row:
                    new_row = original_idx_to_new_excel_row[original_row_idx]
                    if col_name in col_name_to_idx:
                        new_col = col_name_to_idx[col_name]
                        ws_errors.cell(row=new_row, column=new_col).fill = red_fill
            
            output_errors_only = BytesIO()
            wb_errors.save(output_errors_only)
            output_errors_only.seek(0)
            
            files_to_save["error_report"] = (f"æœˆé‡å¡_{sheet_keyword}_ä»…é”™è¯¯è¡Œ_æ ‡çº¢.xlsx", output_errors_only)
            
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆâ€œä»…é”™è¯¯è¡Œâ€æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            
    elapsed = time.time() - start_time
    st.success(f"âœ… {sheet_keyword} æ£€æŸ¥å®Œæˆï¼Œå…± {total_errors} å¤„é”™è¯¯ï¼Œç”¨æ—¶ {elapsed:.2f} ç§’ã€‚")
    
    stats = (total_errors, elapsed, skip_city_manager[0], contracts_seen)
    return stats, files_to_save

# =====================================
# ğŸ•µï¸ (æ–°) æ¼å¡«æ£€æŸ¥å‡½æ•°
# =====================================
def run_leaky_check(zd_df, contract_col_zd, contracts_seen_all_sheets):
    """
    æ‰§è¡Œæ¼å¡«æ£€æŸ¥å¹¶è¿”å› BytesIO æ–‡ä»¶ã€‚
    """
    st.info("â„¹ï¸ æ­£åœ¨æ‰§è¡Œæ¼å¡«æ£€æŸ¥...")
    files_to_save = {}
    
    field_contracts = zd_df[contract_col_zd].dropna().astype(str).str.strip()
    col_car_manager = find_col(zd_df, "æ˜¯å¦è½¦ç®¡å®¶", exact=True)
    col_bonus_type = find_col(zd_df, "ææˆç±»å‹", exact=True)

    missing_contracts_mask = (~field_contracts.isin(contracts_seen_all_sheets))

    if col_car_manager:
        missing_contracts_mask &= ~(zd_df[col_car_manager].astype(str).str.strip().str.lower() == "æ˜¯")
    if col_bonus_type:
        missing_contracts_mask &= ~(
            zd_df[col_bonus_type].astype(str).str.strip().isin(["è”åˆç§Ÿèµ", "é©»åº—"])
        )

    zd_df_missing = zd_df.copy()
    zd_df_missing["æ¼å¡«æ£€æŸ¥"] = ""
    zd_df_missing.loc[missing_contracts_mask, "æ¼å¡«æ£€æŸ¥"] = "â— æ¼å¡«"
    æ¼å¡«åˆåŒæ•° = zd_df_missing["æ¼å¡«æ£€æŸ¥"].eq("â— æ¼å¡«").sum()
    st.warning(f"âš ï¸ å…±å‘ç° {æ¼å¡«åˆåŒæ•°} ä¸ªåˆåŒåœ¨è®°å½•è¡¨ä¸­æœªå‡ºç°ï¼ˆå·²æ’é™¤è½¦ç®¡å®¶ã€è”åˆç§Ÿèµã€é©»åº—ï¼‰")

    # --- å¯¼å‡ºæ–‡ä»¶é€»è¾‘ ---
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

    # æ–‡ä»¶1ï¼šå…¨å­—æ®µè¡¨
    wb = Workbook()
    ws = wb.active
    for r in dataframe_to_rows(zd_df_missing, index=False, header=True):
        ws.append(r)
    
    check_col_idx = -1
    for c_idx, cell in enumerate(ws[1], 1): # éå†è¡¨å¤´
        if cell.value == "æ¼å¡«æ£€æŸ¥":
            check_col_idx = c_idx
            break
            
    if check_col_idx > 0:
        for row in ws.iter_rows(min_row=2, min_col=check_col_idx, max_col=check_col_idx):
            cell = row[0]
            if cell.value == "â— æ¼å¡«":
                cell.fill = yellow_fill

    output_all = BytesIO()
    wb.save(output_all)
    output_all.seek(0)
    files_to_save["leaky_full"] = ("å­—æ®µè¡¨_æ¼å¡«æ ‡æ³¨ç‰ˆ.xlsx", output_all)

    # æ–‡ä»¶2ï¼šä»…æ¼å¡«
    zd_df_only_missing = zd_df_missing[zd_df_missing["æ¼å¡«æ£€æŸ¥"] == "â— æ¼å¡«"].copy()
    if not zd_df_only_missing.empty:
        wb2 = Workbook()
        ws2 = wb2.active
        for r in dataframe_to_rows(zd_df_only_missing, index=False, header=True):
            ws2.append(r)
        
        check_col_idx_2 = -1
        for c_idx, cell in enumerate(ws2[1], 1):
            if cell.value == "æ¼å¡«æ£€æŸ¥":
                check_col_idx_2 = c_idx
                break
        if check_col_idx_2 > 0:
            for row in ws2.iter_rows(min_row=2, min_col=check_col_idx_2, max_col=check_col_idx_2):
                if row[0].value == "â— æ¼å¡«":
                    row[0].fill = yellow_fill

        out2 = BytesIO()
        wb2.save(out2)
        out2.seek(0)
        files_to_save["leaky_only"] = ("å­—æ®µè¡¨_ä»…æ¼å¡«.xlsx", out2)
    else:
        files_to_save["leaky_only"] = (None, None)

    return æ¼å¡«åˆåŒæ•°, files_to_save


# =====================================
# ğŸš€ (æ–°) ç¼“å­˜çš„å®¡æ ¸ä¸»å‡½æ•°
# =====================================
@st.cache_data(show_spinner="æ­£åœ¨æ‰§è¡Œå®¡æ ¸ï¼Œè¯·ç¨å€™...")
def run_full_audit(_uploaded_files):
    """
    æ‰§è¡Œæ‰€æœ‰æ–‡ä»¶è¯»å–ã€é¢„å¤„ç†å’Œæ£€æŸ¥ï¼Œå¹¶è¿”å›æ‰€æœ‰ç»“æœã€‚
    æ­¤å‡½æ•°è¢«ç¼“å­˜ï¼Œåªæœ‰åœ¨ _uploaded_files æ›´æ”¹æ—¶æ‰ä¼šé‡æ–°è¿è¡Œã€‚
    """
    
    # --- 1. ğŸ“– æ–‡ä»¶è¯»å– ---
    # (æ³¨æ„ï¼šfind_file æ¥æ”¶çš„æ˜¯ _uploaded_files)
    main_file = find_file(_uploaded_files, "æœˆé‡å¡")
    fk_file = find_file(_uploaded_files, "æ”¾æ¬¾æ˜ç»†")
    zd_file = find_file(_uploaded_files, "å­—æ®µ")
    ec_file = find_file(_uploaded_files, "äºŒæ¬¡æ˜ç»†")

    fk_df = pd.read_excel(pd.ExcelFile(fk_file), sheet_name=find_sheet(pd.ExcelFile(fk_file), "å¨ç”°"))
    zd_df = pd.read_excel(pd.ExcelFile(zd_file), sheet_name=find_sheet(pd.ExcelFile(zd_file), "é‡å¡"))
    ec_df = pd.read_excel(ec_file)

    contract_col_zd = find_col(zd_df, "åˆåŒ") # ä»…ä¸ºæ¼å¡«æ£€æŸ¥

    # --- 2. ğŸ—ºï¸ æ˜ å°„è¡¨ ---
    mapping_fk = {
        "æˆä¿¡æ–¹": "æˆä¿¡æ–¹",
        "ç§Ÿèµæœ¬é‡‘": "ç§Ÿèµæœ¬é‡‘", 
        "ç§ŸèµæœŸé™": "ç§ŸèµæœŸé™",
        "æŒ‚è½¦å°æ•°": "æŒ‚è½¦æ•°é‡",
        "èµ·ç§Ÿæ”¶ç›Šç‡": "XIRR"
    }
    mapping_zd = {"ä¿è¯é‡‘æ¯”ä¾‹": "ä¿è¯é‡‘æ¯”ä¾‹_2", "é¡¹ç›®ææŠ¥äºº": "ææŠ¥", "èµ·ç§Ÿæ—¶é—´": "èµ·ç§Ÿæ—¥_å•†", "å®¢æˆ·ç»ç†": "å®¢æˆ·ç»ç†_èµ„äº§", "æ‰€å±çœåŒº": "åŒºåŸŸ", "ä¸»è½¦å°æ•°": "ä¸»è½¦å°æ•°", "åŸå¸‚ç»ç†": "åŸå¸‚ç»ç†"}
    mapping_ec = {"äºŒæ¬¡æ—¶é—´": "å‡ºæœ¬æµç¨‹æ—¶é—´"}

    mappings_all = {
        'fk': (mapping_fk, None), # DF ç¨åå¡«å……
        'zd': (mapping_zd, None),
        'ec': (mapping_ec, None),
    }

    # --- 3. ğŸš€ é¢„å¤„ç† ---
    st.info("â„¹ï¸ æ­£åœ¨é¢„å¤„ç†å‚è€ƒæ•°æ®...")
    fk_std = prepare_ref_df(fk_df, mapping_fk, 'fk')
    zd_std = prepare_ref_df(zd_df, mapping_zd, 'zd')
    ec_std = prepare_ref_df(ec_df, mapping_ec, 'ec')

    ref_dfs_std_dict = {
        'fk': fk_std,
        'zd': zd_std,
        'ec': ec_std
    }
    
    # å¡«å…… mappings_all
    mappings_all['fk'] = (mapping_fk, fk_std)
    mappings_all['zd'] = (mapping_zd, zd_std)
    mappings_all['ec'] = (mapping_ec, ec_std)
    st.success("âœ… å‚è€ƒæ•°æ®é¢„å¤„ç†å®Œæˆã€‚")
    
    # --- 4. ğŸ§¾ å¤šsheetå¾ªç¯ ---
    sheet_keywords = ["äºŒæ¬¡", "éƒ¨åˆ†æ‹…ä¿", "éšå·", "é©»åº—å®¢æˆ·"]
    total_all = elapsed_all = skip_total = 0
    contracts_seen_all_sheets = set()
    
    all_generated_files = [] # å­˜å‚¨æ‰€æœ‰ (æ–‡ä»¶å, BytesIO) å…ƒç»„

    for kw in sheet_keywords:
        stats, files_dict = check_one_sheet(kw, main_file, ref_dfs_std_dict, mappings_all)
        (count, used, skipped, seen) = stats
        
        # æ”¶é›†æ–‡ä»¶
        all_generated_files.append(files_dict["full_report"])
        if files_dict["error_report"][0] is not None:
            all_generated_files.append(files_dict["error_report"])
        
        total_all += count
        elapsed_all += used or 0
        skip_total += skipped
        contracts_seen_all_sheets.update(seen)

    # --- 5. ğŸ•µï¸ æ¼å¡«æ£€æŸ¥ ---
    æ¼å¡«åˆåŒæ•°, leaky_files_dict = run_leaky_check(
        zd_df, contract_col_zd, contracts_seen_all_sheets
    )
    
    all_generated_files.append(leaky_files_dict["leaky_full"])
    if leaky_files_dict["leaky_only"][0] is not None:
        all_generated_files.append(leaky_files_dict["leaky_only"])

    # --- 6. è¿”å›æ‰€æœ‰ç»“æœ ---
    stats_summary = {
        "total_all": total_all,
        "elapsed_all": elapsed_all,
        "æ¼å¡«åˆåŒæ•°": æ¼å¡«åˆåŒæ•°
    }
    
    return all_generated_files, stats_summary

# =====================================
# ğŸ åº”ç”¨æ ‡é¢˜ä¸è¯´æ˜ (é‡æ„ç‰ˆ)
# =====================================
st.title("ğŸ“Š æ¨¡æ‹Ÿäººäº‹ç”¨è–ªèµ„è®¡ç®—è¡¨è‡ªåŠ¨å®¡æ ¸ç³»ç»Ÿ-1é‡å¡")
st.image("image/app1(1).png")

# =====================================
# ğŸ“‚ ä¸Šä¼ æ–‡ä»¶åŒºï¼šè¦æ±‚ä¸Šä¼  4 ä¸ª xlsx æ–‡ä»¶ (é‡æ„ç‰ˆ)
# =====================================
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼ æ–‡ä»¶åä¸­åŒ…å«ä»¥ä¸‹å­—æ®µçš„æ–‡ä»¶ï¼šæœˆé‡å¡ã€æ”¾æ¬¾æ˜ç»†ã€å­—æ®µã€äºŒæ¬¡æ˜ç»†ã€‚æœ€åèªŠå†™ï¼Œéœ€æ£€çš„è¡¨ä¸ºæ–‡ä»¶ååŒ…å«â€˜æœˆé‡å¡â€™å­—æ®µçš„è¡¨ã€‚",
    type="xlsx",
    accept_multiple_files=True
)

# =====================================
# ğŸš€ (æ–°) ä¸»æ‰§è¡Œé€»è¾‘
# =====================================
if not uploaded_files or len(uploaded_files) < 4:
    st.warning("âš ï¸ è¯·ä¸Šä¼ æ‰€æœ‰ 4 ä¸ªæ–‡ä»¶åç»§ç»­")
    st.stop()
else:
    st.success("âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ")
    
    # 1. (æ–°) è°ƒç”¨ç¼“å­˜çš„å®¡æ ¸å‡½æ•°
    try:
        all_files, stats = run_full_audit(uploaded_files)

        # 2. (æ–°) æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
        st.success(f"ğŸ¯ å…¨éƒ¨å®¡æ ¸å®Œæˆï¼Œå…± {stats['total_all']} å¤„é”™è¯¯ï¼Œæ€»è€—æ—¶ {stats['elapsed_all']:.2f} ç§’ã€‚")
        st.warning(f"âš ï¸ å…±å‘ç° {stats['æ¼å¡«åˆåŒæ•°']} ä¸ªåˆåŒåœ¨è®°å½•è¡¨ä¸­æœªå‡ºç°ï¼ˆå·²æ’é™¤è½¦ç®¡å®¶ã€è”åˆç§Ÿèµã€é©»åº—ï¼‰")
        
        # 3. (æ–°) â€œé‡æ–°å®¡æ ¸â€æŒ‰é’®
        st.info("ç‚¹å‡»ä¸‹è½½æŒ‰é’®ä¸ä¼šé‡æ–°å®¡æ ¸ã€‚å¦‚éœ€ä½¿ç”¨æ–°æ–‡ä»¶æˆ–å¼ºåˆ¶é‡æ–°è¿è¡Œï¼Œè¯·ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ã€‚")
        if st.button("ğŸ”„ å¼ºåˆ¶é‡æ–°å®¡æ ¸ (æ¸…é™¤ç¼“å­˜)"):
            # æ‰‹åŠ¨æ¸…é™¤ç¼“å­˜
            run_full_audit.clear()
            # å¼ºåˆ¶ Streamlit é‡æ–°è¿è¡Œæ•´ä¸ªè„šæœ¬
            st.rerun()

        # 4. (æ–°) æ˜¾ç¤ºæ‰€æœ‰ä¸‹è½½æŒ‰é’®
        st.divider()
        st.subheader("ğŸ“¤ ä¸‹è½½å®¡æ ¸ç»“æœæ–‡ä»¶")
        
        for (filename, data) in all_files:
            if filename and data: # ç¡®ä¿æ–‡ä»¶åå’Œæ•°æ®éƒ½å­˜åœ¨
                st.download_button(
                    label=f"ğŸ“¥ ä¸‹è½½ {filename}",
                    data=data,
                    file_name=filename,
                    key=f"download_btn_{filename}" # ä½¿ç”¨å”¯ä¸€key
                )
        
        st.success("âœ… æ‰€æœ‰æ£€æŸ¥ã€æ ‡æ³¨ä¸å¯¼å‡ºå®Œæˆï¼")
        
    except FileNotFoundError as e:
        st.error(f"âŒ æ–‡ä»¶æŸ¥æ‰¾å¤±è´¥: {e}")
        st.info("è¯·ç¡®ä¿æ‚¨ä¸Šä¼ äº†æ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶ï¼ˆæœˆé‡å¡ã€æ”¾æ¬¾æ˜ç»†ã€å­—æ®µã€äºŒæ¬¡æ˜ç»†ï¼‰ã€‚")
    except ValueError as e:
        st.error(f"âŒ Sheet æŸ¥æ‰¾å¤±è´¥: {e}")
        st.info("è¯·ç¡®ä¿æ‚¨çš„Excelæ–‡ä»¶åŒ…å«å¿…éœ€çš„ sheetï¼ˆä¾‹å¦‚ 'å¨ç”°', 'é‡å¡'ï¼‰ã€‚")
    except Exception as e:
        st.error(f"âŒ å®¡æ ¸è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        st.exception(e)
